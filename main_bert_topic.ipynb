{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŸå§‹æ•°æ®åŠ è½½ï¼ˆdocsï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰ˆæœ¬æ§åˆ¶\n",
    "version = 'V3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»»åŠ¡è¯´æ˜ï¼ˆä»¥æ–‡çŒ®æ•°æ®å¤„ç†ä¸ºä¾‹ï¼‰ï¼š\n",
    "- 1.å°†EXCELæ•°æ®è½¬æ¢ä¸ºjsonæ ¼å¼ï¼ŒæŒ‰ç…§3TI ã€1ABå’Œ2KEY WORDSï¼Œå®Œæˆæ–‡æœ¬çš„å­—ç¬¦ä¸²æ‹¼æ¥ï¼Œjsonæ–‡ä»¶ä¸­éœ€è¦`year`ã€`title`ã€`abstract`ã€`keywords`ä»¥åŠ`combined_text`å­—æ®µï¼›ï¼ˆAIåº”è¯¥å¾ˆå®¹æ˜“å†™å‡ºæ¥ç›¸åº”çš„è½¬æ¢ä»£ç ï¼Œè¯¥æ–‡ä»¶ä½œä¸ºåŸå§‹ä¸»é¢˜å»ºæ¨¡çš„åŸå§‹æ•°æ®æ–‡ä»¶ï¼‰ï¼›\n",
    "- 2.å˜é‡`docs`æ˜¯æ¨¡å‹çš„è¾“å…¥ï¼Œå°†jsonæ–‡ä»¶ä¸­çš„`combined_text`ä¼ å…¥åˆ°è¯¥å‘é‡ä¸­å½¢æˆä¸€ä¸ªåˆ—è¡¨å‹æ•°æ®ï¼›\n",
    "- 3.æ ¹æ®åç»­ä»£ç è·‘é€šè¿™ä¸€å¥—BERTopicæŠ€æœ¯è·¯çº¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®\n",
    "with open('data\\patents_zf.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# æ‰“å°å‰ä¸¤æ¡æ•°æ®è¿›è¡Œæ£€æŸ¥\n",
    "for item in data[-2:]:\n",
    "    print(item['year'])\n",
    "    #print(item['title'])\n",
    "    #print(item['abstract'])\n",
    "    print(item['combined_text'])\n",
    "\n",
    "# æ–‡æœ¬\n",
    "docs = [item['combined_text'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('å…±è·å–æ•°æ®æ€»é‡:', len(docs),'æ¡')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆ›å»ºæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æœ€æ¨èçš„è‹±æ–‡åµŒå…¥æ¨¡å‹\n",
    "EMBEDDING_MODELS = {\n",
    "    # ğŸ”¥ å¼ºçƒˆæ¨èï¼šä¸“é—¨ä¸ºå¥å­ç›¸ä¼¼åº¦è®­ç»ƒ\n",
    "    'all-MiniLM-L6-v2': {\n",
    "        'name': 'all-MiniLM-L6-v2',\n",
    "        'description': 'è½»é‡é«˜æ•ˆï¼Œå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡',\n",
    "        'size': '80MB',\n",
    "        'dimension': 384\n",
    "    },\n",
    "    \n",
    "    'all-mpnet-base-v2': {\n",
    "        'name': 'all-mpnet-base-v2',\n",
    "        'description': 'æ•ˆæœæœ€å¥½ï¼ŒSOTAè´¨é‡',\n",
    "        'size': '420MB',\n",
    "        'dimension': 768\n",
    "    },\n",
    "    \n",
    "    'paraphrase-MiniLM-L6-v2': {\n",
    "        'name': 'paraphrase-MiniLM-L6-v2',\n",
    "        'description': 'æ“…é•¿è¯­ä¹‰ç›¸ä¼¼åº¦',\n",
    "        'size': '80MB',\n",
    "        'dimension': 384\n",
    "    },\n",
    "    \n",
    "    # é€šç”¨çš„BERTæ¨¡å‹\n",
    "    'bert-base-nli-mean-tokens': {\n",
    "        'name': 'bert-base-nli-mean-tokens',\n",
    "        'description': 'è‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡è®­ç»ƒ',\n",
    "        'size': '440MB',\n",
    "        'dimension': 768\n",
    "    },\n",
    "    \n",
    "    # ç§‘å­¦/å­¦æœ¯ä¸“ç”¨\n",
    "    'all-MiniLM-L12-v2': {\n",
    "        'name': 'all-MiniLM-L12-v2',\n",
    "        'description': '12å±‚ç‰ˆæœ¬ï¼Œæ›´æ·±çš„è¡¨ç¤º',\n",
    "        'size': '120MB',\n",
    "        'dimension': 384\n",
    "    },\n",
    "    \n",
    "    # å¤šè¯­è¨€ï¼ˆå¦‚æœåŒ…å«å…¶ä»–è¯­è¨€ï¼‰\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2': {\n",
    "        'name': 'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "        'description': 'å¤šè¯­è¨€æ”¯æŒ',\n",
    "        'size': '470MB',\n",
    "        'dimension': 384\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 1. è®¾ç½®å›½å†…é•œåƒæº\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "from bertopic import BERTopic\n",
    "import torch # æ–°å¢ï¼šå¯¼å…¥PyTorchï¼ˆsentence_transformersçš„åº•å±‚ï¼‰\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- æ ¸å¿ƒï¼šæ£€æŸ¥å¹¶æŒ‡å®šè®¾å¤‡ ---\n",
    "# æ£€æŸ¥CUDAï¼ˆå³NVIDIA GPUæ”¯æŒï¼‰æ˜¯å¦å¯ç”¨\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPUå¯ç”¨ï¼Œæ­£åœ¨ä½¿ç”¨: {torch.cuda.get_device_name(0)}\")\n",
    "    # å¯é€‰ï¼šå¦‚æœä½ æƒ³ä½¿ç”¨ç‰¹å®šçš„GPUï¼ˆå¦‚ç¬¬0å—ï¼‰\n",
    "    # device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸  æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œå°†ä½¿ç”¨CPUã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__) # æŸ¥çœ‹PyTorchç‰ˆæœ¬\n",
    "print(torch.cuda.is_available()) # æ ¸å¿ƒï¼šæŸ¥çœ‹CUDAæ˜¯å¦å¯ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# import os\n",
    "\n",
    "# # 1. æŒ‡å®šä½ å¸Œæœ›ä¿å­˜æ¨¡å‹çš„æœ¬åœ°ç›®å½•\n",
    "# local_model_path = \"./my_models/all-mpnet-base-v2\"\n",
    "\n",
    "# # 2. æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰æ¨¡å‹ï¼Œæ²¡æœ‰åˆ™ä¸‹è½½å¹¶ä¿å­˜\n",
    "# if not os.path.exists(local_model_path):\n",
    "#     print(\"æ¨¡å‹ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä¸‹è½½å¹¶ä¿å­˜åˆ°æœ¬åœ°...\")\n",
    "#     model = SentenceTransformer('all-mpnet-base-v2')\n",
    "#     embedding_model.save(local_model_path)  # å…³é”®ï¼šä¿å­˜åˆ°æŒ‡å®šè·¯å¾„\n",
    "#     print(f\"æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{local_model_path}\")\n",
    "# else:\n",
    "#     print(\"æ¨¡å‹å·²å­˜åœ¨ï¼Œä»æœ¬åœ°åŠ è½½...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ä¹‹åéƒ½ä»è¿™ä¸ªè·¯å¾„åŠ è½½ï¼Œå®Œå…¨æ— éœ€ç½‘ç»œ\n",
    "embedding_model = SentenceTransformer(f\"./my_models/all-mpnet-base-{version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. è®¡ç®—å¹¶ä¿å­˜è¯å‘é‡\n",
    "def compute_and_save_embeddings(docs, save_path='embeddings_test'):\n",
    "    \"\"\"è®¡ç®—å¹¶ä¿å­˜è¯å‘é‡\"\"\"\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"åŠ è½½å·²ä¿å­˜çš„åµŒå…¥: {save_path}\")\n",
    "        with open(save_path, 'rb') as f:\n",
    "            embeddings = pickle.load(f)\n",
    "    else:\n",
    "        print(\"è®¡ç®—æ–°çš„åµŒå…¥...\")\n",
    "        embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "        print(f\"åµŒå…¥å·²ä¿å­˜åˆ°: {save_path}\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    \"\"\"ä»JSONæ–‡ä»¶åŠ è½½åœç”¨è¯åˆ—è¡¨\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            stopwords = json.load(f)\n",
    "        print(f\"å·²åŠ è½½ {len(stopwords['common'])} ä¸ªåœç”¨è¯\")\n",
    "        return stopwords['common']  # è½¬æ¢ä¸ºé›†åˆæé«˜æŸ¥æ‰¾æ•ˆç‡\n",
    "    except FileNotFoundError:\n",
    "        print(f\"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨\")\n",
    "        return set()\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼\")\n",
    "        return set()\n",
    "\n",
    "# # æµ‹è¯•ç¤ºä¾‹\n",
    "# stopwords = load_stopwords('data\\stopwords.json')\n",
    "# print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAPï¼ˆé™ç»´æ¨¡å‹ï¼‰å’ŒHDBSCANï¼ˆèšç±»æ¨¡å‹ï¼‰å‚æ•°è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. åˆ›å»ºUMAPé™ç»´æ¨¡å‹\n",
    "# umap_model = UMAP(\n",
    "#   n_neighbors=20, # æ§åˆ¶å±€éƒ¨ä¸å…¨å±€ç»“æ„çš„å¹³è¡¡,å†³å®šæ¯ä¸ªç‚¹è¦è€ƒè™‘å¤šå°‘ä¸ªæœ€è¿‘é‚»å±…,å€¼è¶Šå¤§ï¼ˆå¦‚15-200ï¼‰ï¼šæ›´å…³æ³¨å…¨å±€ç»“æ„ï¼Œè·å¾—æ›´å®è§‚çš„è§†å›¾;å€¼è¶Šå°ï¼ˆå¦‚5-15ï¼‰ï¼šæ›´å…³æ³¨å±€éƒ¨ç»“æ„ï¼Œæ•æ‰æ›´å¤šç»†èŠ‚ã€‚\n",
    "#   n_components=5, # é™ç»´åçš„ç»´åº¦ï¼Œé€šå¸¸è®¾ç½®ä¸º2æˆ–3ä»¥ä¾¿å¯è§†åŒ–ï¼Œä½†BERTopicä¸­å»ºè®®è®¾ç½®ä¸º5ä»¥ä¿ç•™æ›´å¤šä¿¡æ¯\n",
    "#   min_dist=0.0, # æ§åˆ¶åµŒå…¥ç©ºé—´ä¸­ç‚¹ä¹‹é—´çš„æœ€å°è·ç¦»,å€¼è¶Šå°ï¼ˆå¦‚0.0-0.1ï¼‰ï¼šåµŒå…¥ç©ºé—´æ›´ç´§å‡‘ï¼Œç‚¹ä¹‹é—´è·ç¦»æ›´è¿‘;å€¼è¶Šå¤§ï¼ˆå¦‚0.5-0.99ï¼‰ï¼šåµŒå…¥ç©ºé—´æ›´ç¨€ç–ï¼Œç‚¹ä¹‹é—´è·ç¦»æ›´è¿œã€‚\n",
    "#   metric='cosine', # è·ç¦»åº¦é‡æ–¹æ³•ï¼Œå¸¸ç”¨çš„æœ‰'cosine'ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰å’Œ'euclidean'ï¼ˆæ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰,æ–‡æœ¬æ•°æ®ã€é«˜ç»´ç¨€ç–æ•°æ®ã€TF-IDFå‘é‡ç­‰é€šå¸¸ä½¿ç”¨'cosine'ï¼Œè€Œä½ç»´å¯†é›†æ•°æ®é€šå¸¸ä½¿ç”¨'euclidean'\n",
    "#   random_state=5  # è®¾ç½®éšæœºç§å­,ç¡®ä¿ç»“æœå¯é‡å¤æ€§,UMAPæœ‰éšæœºåˆå§‹åŒ–è¿‡ç¨‹ï¼Œè®¾ç½®æ­¤å‚æ•°ä¿è¯æ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´\n",
    "# )\n",
    "\n",
    "# # 3. åˆ›å»ºHDBSCANèšç±»æ¨¡å‹\n",
    "# # å¦‚æœè¦å»ºè®¾ç¦»ç¾¤å€¼ï¼Œå¯ä»¥å‡å°ä¸‹é¢ä¸¤ä¸ªå‚æ•°min_cluster_size min_samples\n",
    "# # https://hdbscan.readthedocs.io/en/latest/faq.html\n",
    "# hdbscan_model = HDBSCAN(\n",
    "#   min_cluster_size=50, # æœ€å°èšç±»å¤§å°,å†³å®šäº†å½¢æˆä¸€ä¸ªèšç±»æ‰€éœ€çš„æœ€å°æ•°æ®ç‚¹æ•°é‡ã€‚è¾ƒå¤§çš„å€¼ä¼šå¯¼è‡´æ›´å°‘ä½†æ›´ç¨³å®šçš„èšç±»ï¼Œè€Œè¾ƒå°çš„å€¼ä¼šäº§ç”Ÿæ›´å¤šä½†å¯èƒ½ä¸å¤ªç¨³å®šçš„èšç±»ã€‚\n",
    "# # è¿‡æ»¤æ‰å¤ªå°çš„å™ªå£°ç‚¹èšé›†\n",
    "# # æ§åˆ¶èšç±»ç²’åº¦ï¼šå€¼è¶Šå¤§ï¼Œç”Ÿæˆçš„èšç±»è¶Šå°‘ã€è¶Šç¨³å¥\n",
    "# # è¿™æ˜¯HDBSCANä¸­æœ€é‡è¦çš„å‚æ•°ï¼Œå¯¹ç»“æœå½±å“æœ€å¤§\n",
    "\n",
    "#   min_samples=3, # æœ€å°æ ·æœ¬æ•°,å½±å“èšç±»çš„æ ¸å¿ƒç‚¹å®šä¹‰ã€‚è¾ƒé«˜çš„å€¼ä¼šä½¿å¾—èšç±»æ›´ä¸¥æ ¼ï¼Œå¯èƒ½å¯¼è‡´æ›´å¤šçš„ç¦»ç¾¤ç‚¹ï¼Œè€Œè¾ƒä½çš„å€¼åˆ™æ›´å®½æ¾ï¼Œå¯èƒ½å½¢æˆæ›´å¤šçš„èšç±»ã€‚\n",
    "#   metric='euclidean'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. å®šä¹‰å‚æ•°é…ç½®ä¸­å¿ƒ\n",
    "model_config = {\n",
    "    \"UMAP\": {\n",
    "        \"n_neighbors\": 20,\n",
    "        \"n_components\": 5,\n",
    "        \"min_dist\": 0.0,\n",
    "        \"metric\": 'cosine',\n",
    "        \"random_state\": 5\n",
    "    },\n",
    "    \"HDBSCAN\": {\n",
    "        \"min_cluster_size\": 50,\n",
    "        \"min_samples\": 3,\n",
    "        \"metric\": 'euclidean',\n",
    "        \"prediction_data\": True  # å»ºè®®å¼€å¯ï¼Œä»¥ä¾¿åç»­è¿›è¡Œç»´åº¦è½¬æ¢\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. æ–¹ä¾¿æ‰“å°æ•°å€¼çš„å‡½æ•°\n",
    "def print_config(config):\n",
    "    print(\"=\"*30)\n",
    "    print(\" å½“å‰æ¨¡å‹è¶…å‚æ•°é…ç½® \")\n",
    "    print(\"=\"*30)\n",
    "    for model_name, params in config.items():\n",
    "        print(f\"\\n[{model_name} å‚æ•°]:\")\n",
    "        for key, value in params.items():\n",
    "            # ä½¿ç”¨ ljust è®©è¾“å‡ºå¯¹é½ï¼Œæ›´æ˜“è¯»\n",
    "            print(f\"  - {key.ljust(20)}: {value}\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "# æ‰§è¡Œæ‰“å°\n",
    "print_config(model_config)\n",
    "\n",
    "# 3. å®ä¾‹åŒ–æ¨¡å‹\n",
    "# ä½¿ç”¨ ** ç›´æ¥è§£åŒ…å­—å…¸å‚æ•°\n",
    "umap_model = UMAP(**model_config[\"UMAP\"])\n",
    "hdbscan_model = HDBSCAN(**model_config[\"HDBSCAN\"])\n",
    "\n",
    "print(\"\\næ¨¡å‹å·²æˆåŠŸæ ¹æ®ä¸Šè¿°å‚æ•°åˆå§‹åŒ–ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopicæ¨¡å‹åˆ›å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœç”¨è¯åœ°å€\n",
    "stop_words = load_stopwords('data\\stopwords.json')\n",
    "# åˆ›å»ºCountVectorizeræ¨¡å‹,è‡ªå®šä¹‰åœç”¨è¯\n",
    "vectorizer_model = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# æ­£å¼åˆ›å»ºBERTopicæ¨¡å‹\n",
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. è®¡ç®—å¹¶ä¿å­˜è¯å‘é‡\n",
    "embeddings = compute_and_save_embeddings(docs, 'results\\embedding_results\\embeddings_patents_zf_v1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½ çš„ä»£ç æµç¨‹ï¼š\n",
    "# +-------------------+       +-------------------------+       +---------------+\n",
    "# | ä½ çš„é¢„è®¡ç®—åµŒå…¥     |------>| BERTopic.fit_transform()|------>|æœ€ç»ˆä¸»é¢˜ç»“æœ   |\n",
    "# | embeddings_patents|       |                         |       |               |\n",
    "# | _zf_v1.pkl        |       |                         |       | topics, probs |\n",
    "# +-------------------+       +-------------------------+       +---------------+\n",
    "#                                    â”‚\n",
    "#                                    â–¼ å†…éƒ¨å¤„ç†æµç¨‹\n",
    "#                         +-----------------------+\n",
    "#                         â”‚ 1. æ¥æ”¶ä½ çš„ embeddings â”‚\n",
    "#                         +-----------------------+\n",
    "#                                    â”‚\n",
    "#                                    â–¼\n",
    "#                         +-----------------------+\n",
    "#                         â”‚ 2. âœ… UMAP é™ç»´        â”‚â† è¿™é‡Œä½¿ç”¨ UMAPï¼\n",
    "#                         +-----------------------+\n",
    "#                                    â”‚\n",
    "#                                    â†“\n",
    "#                         +-----------------------+\n",
    "#                         â”‚ 3. HDBSCAN èšç±»        â”‚\n",
    "#                         +-----------------------+\n",
    "#                                    â”‚\n",
    "#                                    â†“\n",
    "#                         +----------------------------------+\n",
    "#                         â”‚ 4. Vectorizer\\c-TF-IDFæå–ä¸»é¢˜è¯ç­‰â”‚\n",
    "#                         +----------------------------------+\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings=embeddings) #ä¼ å…¥è®­ç»ƒå¥½çš„è¯å‘é‡\n",
    "# æŸ¥çœ‹ä¸»é¢˜\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä¿å­˜èšç±»ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ä¿å­˜æ•´ä¸ªä¸»é¢˜æ¨¡å‹\n",
    "# topic_model.save(\"bertopic_patents_zf_amb_v3\")\n",
    "# print(\"ä¸»é¢˜æ¨¡å‹å·²ä¿å­˜åˆ° bertopic_patents_zf_amb_v3\")\n",
    "\n",
    "## ç›´æ¥åŠ è½½åˆå§‹æ¨¡å‹\n",
    "topic_model = BERTopic.load(f\"bertopic_patents_zf_amb{version}\")\n",
    "print(\"åˆå§‹ä¸»é¢˜æ¨¡å‹åŠ è½½æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs = topic_model.get_document_info(docs)\n",
    "with open(r'data/patents_zf.json', 'r', encoding='utf-8') as file:\n",
    "  data = json.load(file)\n",
    "  # å‡è®¾JSONæ˜¯å­—å…¸åˆ—è¡¨\n",
    "abstract = [item['abstract'] for item in data]  # æå–æ–‡æœ¬\n",
    "years = [item['year'] for item in data]  # æå–å¹´ä»½\n",
    "title = [item['title'] for item in data]  # æå–æ ‡é¢˜\n",
    "#author_keywords = [item['author_keywords'] for item in data]  # æå–å…³é”®è¯\n",
    "# æ’å…¥åˆ°èšç±»ç»“æœ\n",
    "\n",
    "topic_docs.insert(1, 'æ ‡é¢˜', title)\n",
    "topic_docs.insert(2, 'å¹´ä»½', years)\n",
    "topic_docs.insert(3, 'æ‘˜è¦', abstract)\n",
    "#topic_docs.insert(4, 'ä½œè€…å…³é”®è¯', author_keywords)\n",
    "topic_docs.to_csv('./patents_zf_Clustering_detailed_result_V3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŸå§‹ä¸»é¢˜å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings = UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings, hide_document_hover=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å±‚æ¬¡èšç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "hierarchical_clustering = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import plotly.io as pio\n",
    "\n",
    "# åˆ›å»ºäº¤äº’å¼å±‚æ¬¡æ ‘\n",
    "hierarchical_fig = topic_model.visualize_hierarchy(\n",
    "    hierarchical_topics=hierarchical_topics,\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# æ˜¾ç¤ºå›¾å½¢\n",
    "hierarchical_fig.show()\n",
    "\n",
    "# ä¿å­˜ä¸ºHTMLä»¥ä¾¿äº¤äº’\n",
    "hierarchical_fig.write_html(f\"hierarchical_topics_{version}.html\")\n",
    "print(f'å·²ç»åˆ›å»ºå±‚æ¬¡æ ‘ï¼Œå¹¶ä¿å­˜è‡³hierarchical_topics_{version}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è®¡ç®—å¯ä»¥è¿›ä¸€æ­¥åˆå¹¶çš„ä¸»é¢˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import fcluster, cut_tree\n",
    "\n",
    "# 1. é’ˆå¯¹æ•°æ®ç»“æ„æå– Scipy é“¾æ¥çŸ©é˜µ\n",
    "def get_linkage_matrix_final(df):\n",
    "    # æå–å·¦å­èŠ‚ç‚¹ã€å³å­èŠ‚ç‚¹ã€è·ç¦»\n",
    "    # ä½ çš„åˆ—åæ˜¯ Child_Left_ID, Child_Right_ID, Distance\n",
    "    left = df['Child_Left_ID'].values.astype(float)\n",
    "    right = df['Child_Right_ID'].values.astype(float)\n",
    "    dist = df['Distance'].values.astype(float)\n",
    "    \n",
    "    # æå–ä¸»é¢˜æ•°é‡ï¼šç”±äºä½ çš„ Topics åˆ—å­˜çš„æ˜¯åˆ—è¡¨ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æ¯ä¸ªåˆ—è¡¨çš„é•¿åº¦\n",
    "    num_topics = df['Topics'].apply(len).values.astype(float)\n",
    "    \n",
    "    # åˆå¹¶ä¸º Scipy éœ€è¦çš„ Linkage Matrix (n-1, 4)\n",
    "    return np.column_stack([left, right, dist, num_topics])\n",
    "\n",
    "# 2. æ‰§è¡ŒçŸ©é˜µè½¬æ¢\n",
    "# æ³¨æ„ï¼šScipy è¦æ±‚çŸ©é˜µæŒ‰è·ç¦»ä»å°åˆ°å¤§æ’åºï¼ŒBERTopic è¿”å›çš„é¡ºåºé€šå¸¸æ˜¯åçš„ï¼ˆæ ¹èŠ‚ç‚¹åœ¨æœ€å‰ï¼‰\n",
    "df_sorted = hierarchical_topics.sort_values('Distance')\n",
    "linkage_matrix = get_linkage_matrix_final(df_sorted)\n",
    "\n",
    "# 3. è¿›è¡Œåˆ†ç»„\n",
    "# n_groups æ˜¯ä½ æƒ³è¦çš„åˆ†ç»„æ•°é‡ï¼Œä¾‹å¦‚ 20 ç»„------------------------------------------------------\n",
    "n_groups = 20 \n",
    "clusters = cut_tree(linkage_matrix, n_clusters=n_groups).flatten()\n",
    "\n",
    "# 4. æ˜ å°„å›åŸå§‹ä¸»é¢˜æ ‡ç­¾\n",
    "# è·å–æ‰€æœ‰åŸå§‹ä¸»é¢˜ IDï¼ˆæ’å¥½åºï¼Œç¡®ä¿ä¸ cut_tree ç»“æœå¯¹åº”ï¼‰\n",
    "all_topics = sorted([t for t in topic_model.get_topics().keys() if t != -1])\n",
    "\n",
    "df_res = pd.DataFrame({\n",
    "    \"topic_id\": all_topics,\n",
    "    \"group\": clusters\n",
    "})\n",
    "\n",
    "# 5. åŠ ä¸Šè¯­ä¹‰æ ‡ç­¾æ–¹ä¾¿æŸ¥çœ‹\n",
    "topic_labels = topic_model.generate_topic_labels(nr_words=3, separator=\"_\")\n",
    "id_to_label = {int(l.split(\"_\")[0]): l for l in topic_labels if not l.startswith(\"-1\")}\n",
    "df_res['label'] = df_res['topic_id'].map(id_to_label)\n",
    "\n",
    "# æŸ¥çœ‹æœ€ç»ˆç»“æœ\n",
    "print(df_res.head(20))\n",
    "\n",
    "# æ£€æŸ¥æ¯ä¸ªç»„åŒ…å«çš„ä¸»é¢˜æ•°\n",
    "print(\"\\næ¯ç»„ä¸»é¢˜æ•°é‡åˆ†å¸ƒï¼š\")\n",
    "print(df_res['group'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = f\"\"\"\n",
    "# è¯·æŒ‰ç…§å›½é˜²ç§‘æŠ€ä¸“å®¶è§’è‰²ï¼Œå°†ä»¥ä¸‹å…³é”®è¯åˆ—è¡¨è½¬åŒ–ä¸º JSON æ ¼å¼çš„ä¸»é¢˜æ˜ å°„ã€‚\n",
    "# è¦æ±‚è¾“å‡ºæ ¼å¼ï¼š\n",
    "# {{\n",
    "#     0: \"ä¸»é¢˜åç§°\",\n",
    "#     1: \"ä¸»é¢˜åç§°\"\n",
    "# }}\n",
    "\n",
    "# å¾…å¤„ç†æ•°æ®ï¼š\n",
    "# {input_keywords}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. æŒ‰ç…§ group åˆ†ç»„ï¼Œå¹¶å°† topic_id èšåˆä¸ºåˆ—è¡¨\n",
    "grouped_topics = df_res.groupby('group')['topic_id'].apply(list).reset_index()\n",
    "\n",
    "# 2. æ ¼å¼åŒ–è¾“å‡º\n",
    "print(f\"å…±è¯†åˆ«å‡º {len(grouped_topics)} ä¸ªä¸»é¢˜ç»„ï¼š\\n\")\n",
    "\n",
    "for _, row in grouped_topics.iterrows():\n",
    "    # å°† group ç¼–å·åŠ  1 æ–¹ä¾¿é˜…è¯»ï¼ˆä»ç¬¬ä¸€ç»„å¼€å§‹è®¡æ•°ï¼‰\n",
    "    group_num = row['group'] + 1\n",
    "    topic_list = row['topic_id']\n",
    "    \n",
    "    # æ‰“å°æ ¼å¼ï¼šç¬¬ä¸€ç»„ï¼š[0, 1, 2, ...]\n",
    "    print(f\"ç¬¬{group_num}ç»„ï¼š{topic_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. å¦‚æœä½ æƒ³åŒæ—¶çœ‹åˆ°è¿™ç»„ä¸»é¢˜çš„å…³é”®è¯æ ‡ç­¾ï¼ˆæ–¹ä¾¿éªŒè¯è¯­ä¹‰ä¸€è‡´æ€§ï¼‰\n",
    "print(\"\\n\" + \"=\"*30 + \"\\nè¯¦ç»†è¯­ä¹‰åˆ†ç»„ï¼š\\n\")\n",
    "for _, row in grouped_topics.iterrows():\n",
    "    group_num = row['group'] + 1\n",
    "    # è·å–è¯¥ç»„ä¸‹æ‰€æœ‰ä¸»é¢˜çš„æ ‡ç­¾\n",
    "    labels = df_res[df_res['group'] == row['group']]['label'].tolist()\n",
    "    print(f\"ç¬¬{group_num}ç»„ åŒ…å«ä¸»é¢˜ï¼š\")\n",
    "    for lbl in labels:\n",
    "        print(f\"  - {lbl}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. æå–åµŒå¥—åˆ—è¡¨æ ¼å¼ï¼š[[ç»„1åºå·], [ç»„2åºå·], ...]\n",
    "nested_topic_lists = grouped_topics['topic_id'].tolist()\n",
    "\n",
    "print(\"--- åµŒå¥—åˆ—è¡¨è¾“å‡º ---\")\n",
    "print(nested_topic_lists)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. åˆå¹¶åçš„ä¸»é¢˜ç†è§£æ–¹æ³•ä¸€ï¼šæå–æ¯ç»„çš„ä»£è¡¨æ€§å…³é”®è¯ï¼ˆä¸¢å¤±c-tf-idfæƒé‡ï¼‰\n",
    "# é€»è¾‘ï¼šè·å–æ¯ç»„å†…æ‰€æœ‰ä¸»é¢˜çš„å…³é”®è¯ï¼Œè®¡ç®—è¯é¢‘ï¼Œå–å‰ 10 ä¸ªä½œä¸ºç»„å…³é”®è¯\n",
    "group_keywords = {}\n",
    "\n",
    "for _, row in grouped_topics.iterrows():\n",
    "    group_num = row['group']\n",
    "    topics_in_group = row['topic_id']\n",
    "    \n",
    "    all_words_in_group = []\n",
    "    for t_id in topics_in_group:\n",
    "        # è·å–å•ä¸ªä¸»é¢˜çš„å…³é”®è¯ (è¿”å›çš„æ˜¯ [(word, score), ...])\n",
    "        words = [word for word, _ in topic_model.get_topic(t_id)]\n",
    "        all_words_in_group.extend(words)\n",
    "    \n",
    "    # ç»Ÿè®¡è¯é¢‘å¹¶å–å‰ 10 ä¸ªæœ€èƒ½ä»£è¡¨æœ¬ç»„çš„è¯\n",
    "    from collections import Counter\n",
    "    # è¿‡æ»¤æ‰ä¸€äº›æå…¶å¸¸è§çš„é‡å¤è¯ï¼Œä¿ç•™æ¯ç»„ç‹¬ç‰¹çš„è¯­ä¹‰\n",
    "    most_common_words = [word for word, count in Counter(all_words_in_group).most_common(10)]\n",
    "    group_keywords[group_num + 1] = most_common_words\n",
    "\n",
    "# 3. æ ¼å¼åŒ–å±•ç¤ºç»„å…³é”®è¯\n",
    "print(\"--- å„ç»„ä»£è¡¨æ€§å…³é”®è¯ ---\")\n",
    "for g_id, keywords in group_keywords.items():\n",
    "    print(f\"ç¬¬{g_id}ç»„å…±åŒç‰¹å¾è¯: {', '.join(keywords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆå¹¶ä¸»é¢˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nested_topic_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.merge_topics(docs, [\n",
    "#     [9,10,7,13,37,33,29,41,20,22],\n",
    "#     [50,6,38,19,14,16,54],\n",
    "#     [18,51,40,28,52,31,24],\n",
    "#     [3,23,47],\n",
    "#     [36,2,39,5,42,11,15,17,0,21,27,25,48,53,34,4,30,46,45,12,44,55],\n",
    "#     [49,56,1,8,32,26],\n",
    "#     [43,35]\n",
    "# ])\n",
    "\n",
    "# åˆå¹¶ä¸»é¢˜å¹¶æ˜¾ç¤º\n",
    "topic_model.merge_topics(docs, nested_topic_lists)\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs = topic_model.get_document_info(docs)\n",
    "topic_docs.to_csv(f'./patents_zf_merged20èšç±»ç»“æœ{version}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings = UMAP(n_neighbors=20, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings, hide_document_hover=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å–æ‰€æœ‰ä¸»é¢˜çš„å­—å…¸ {topic_id: [(word, score), ...]}\n",
    "all_topics_dict = topic_model.get_topics()\n",
    "\n",
    "print(\"ç´¢å¼•ï¼šå…³é”®è¯åç§°\")\n",
    "# éå†å­—å…¸å¹¶æ’åº\n",
    "for topic_id in sorted(all_topics_dict.keys()):\n",
    "    if topic_id != -1:  # æ’é™¤ç¦»ç¾¤å€¼ä¸»é¢˜\n",
    "        # æå–å•è¯å¹¶ç”¨é€—å·è¿æ¥\n",
    "        words = [word for word, score in all_topics_dict[topic_id]]\n",
    "        words_str = \", \".join(words)\n",
    "        print(f\"{topic_id}ï¼š{words_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†å¤‡è‡ªå®šä¹‰æ ‡é¢˜åˆ—è¡¨\n",
    "custom_titles = [f\"æ ‡é¢˜_{i}: {doc}\" for i, doc in enumerate(docs)]\n",
    "# ä½¿ç”¨è‡ªå®šä¹‰æ ‡ç­¾ï¼ˆéœ€è¦å…ˆè®¾ç½®ï¼‰\n",
    "topic_model.set_topic_labels({\n",
    "    0: \"1.æ­¦å™¨è£…å¤‡ç»“æ„è®¾è®¡ä¸åˆ¶å›¾\", \n",
    "    1: \"2.åŠå¯¼ä½“å·¥è‰ºä¸å¾®ç”µå­å™¨ä»¶\", \n",
    "    2: \"3.å†›äº‹ä¿¡æ¯ç³»ç»Ÿä¸æ•°æ®å®‰å…¨\",\n",
    "    3: \"4.èƒ½æºåŠ¨åŠ›ä¸å…ˆè¿›æ¨è¿›ç³»ç»Ÿ\",\n",
    "    4: \"5.ç”Ÿç‰©é˜²å¾¡ä¸åŸºå› é¶å‘æŠ€æœ¯\",\n",
    "    5: \"6.æˆ˜åœºåŒ»ç–—ä¸å®˜å…µå¥åº·ç›‘æµ‹\",\n",
    "    6: \"7.é«˜æ€§èƒ½å«èƒ½ææ–™ä¸åŒ–å­¦é˜²å¾¡\",\n",
    "    7: \"8.å®šå‘èƒ½æ­¦å™¨ä¸æ¿€å…‰å…‰ç”µå¯¹æŠ—\",\n",
    "    8: \"9.å¾®çº³æµæ§ä¸ç”ŸåŒ–ä¼ æ„Ÿä¾¦å¯Ÿ\",\n",
    "    9: \"10.ç–«è‹—ç ”å‘ä¸ç—…æ¯’è½½ä½“é˜²å¾¡\",\n",
    "    10: \"11.æ— äººç³»ç»Ÿæ„ŸçŸ¥ä¸æ¨¡æ‹Ÿä»¿çœŸ\",\n",
    "    11: \"12.é‡å¤–æˆ˜æœ¯ç”µæºä¸èƒ½é‡ç®¡ç†\",\n",
    "    12: \"13.èˆªç©ºèˆªå¤©å¤åˆææ–™ä¸éšèº«æ¶‚å±‚\",\n",
    "    13: \"14.ç£ç‰¹æ€§ç›‘æµ‹ä¸æˆ˜ç•¥æ ¸ç£æŠ€æœ¯\",\n",
    "    14: \"15.çƒ­èƒ½å¾ªç¯ä¸èˆªç©ºç‡ƒæ²¹ç®¡ç†\",\n",
    "    15: \"16.é›·è¾¾ä¾¦å¯Ÿä¸ç”µå­æˆ˜å¯¹æŠ—\",\n",
    "    16: \"17.è„‘æ§æŠ€æœ¯ä¸ç¥ç»é€šä¿¡æ¥å£\",\n",
    "    17: \"18.æ ¸ç”ŸåŒ–æ¢æµ‹ä¸è¾å°„æˆåƒæˆåƒ\",\n",
    "    18: \"19.èˆªç©ºå‘åŠ¨æœºä¸æ¶¡è½®å¶ç‰‡æŠ€æœ¯\",\n",
    "    19: \"20.æˆ˜ä¼¤ä¿®å¤ä¸ç‰¹ç§ç”Ÿç‰©ææ–™\"\n",
    "})\n",
    "\n",
    "# åœ¨å¯è§†åŒ–ä¸­ä¼ é€’è‡ªå®šä¹‰æ ‡é¢˜\n",
    "fig = topic_model.visualize_documents(\n",
    "    docs=docs,\n",
    "    reduced_embeddings=reduced_embeddings,\n",
    "    custom_labels=custom_titles,  # ä½¿ç”¨è‡ªå®šä¹‰æ ‡ç­¾\n",
    "    hide_document_hover=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ›´æ–°å­—ä½“æ ·å¼\n",
    "fig.update_layout(\n",
    "    title=\"GFä¸»é¢˜åˆ†å¸ƒæƒ…å†µ\",\n",
    "    # å±…ä¸­æ ‡é¢˜\n",
    "    title_x=0.5,\n",
    "    font=dict(\n",
    "        family=\"KaiTi\",  # ä¸­æ–‡å­—ä½“\n",
    "        size=20,\n",
    "        color=\"black\",\n",
    "        weight=\"bold\"  # è®¾ç½®å­—ä½“ç²—ç»†\n",
    "    ),\n",
    "    title_font=dict(\n",
    "        family=\"KaiTi\",\n",
    "        size=40,\n",
    "        color=\"black\",\n",
    "        weight=\"bold\"  # è®¾ç½®å­—ä½“ç²—ç»†\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_in_group ä¸»é¢˜æ•°\n",
    "# n_words ä»£è¡¨è¯æ•°\n",
    "# height/width æ¯ä¸ªå­å›¾çš„é«˜/å®½\n",
    "fig = topic_model.visualize_barchart(top_n_topics=20,n_words=10,height=500, width = 300,custom_labels=custom_titles) # ä¹Ÿå¯ä»¥è®¾ç½® n_words æŒ‡å®šè¯æ•°\n",
    "fig.show()\n",
    "fig.write_html(f'topic_words_merged_scores{version}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ—¶åºä¸»é¢˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ä½¿ç”¨è‡ªå®šä¹‰æ ‡ç­¾ï¼ˆéœ€è¦å…ˆè®¾ç½®ï¼‰\n",
    "# topic_model.set_topic_labels({\n",
    "#     0: \"åŸºç¡€ä¿¡æ¯æ£€ç´¢ç†è®ºä¸æ–¹æ³•\", \n",
    "#     1: \"äººå·¥æ™ºèƒ½ä¸å­¦æœ¯ä¿¡æ¯åº”ç”¨\", \n",
    "#     2: \"ç½‘ç»œå®‰å…¨ä¸éšç§å¢å¼ºæ£€ç´¢\",\n",
    "#     3: \"é¢†åŸŸçŸ¥è¯†ç»„ç»‡ä¸è¯­ä¹‰æ£€ç´¢\",\n",
    "#     4: \"å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢\",\n",
    "#     5: \"é—®ç­”ç³»ç»Ÿä¸çŸ¥è¯†æ¨ç†\",\n",
    "#     6: \"é‡å­ä¸ç‰©ç†å¯å‘çš„æ£€ç´¢æ¨¡å‹\",\n",
    "# })\n",
    "# import pandas as pd\n",
    "\n",
    "# è¯»å–CSVæ–‡ä»¶\n",
    "#df = pd.read_csv('patents_zf_Clustering_result_V2.csv')  # å¦‚æœæ–‡ä»¶æ˜¯CSVæ ¼å¼\n",
    "# æˆ–è€…å¦‚æœæ–‡ä»¶æ˜¯å…¶ä»–æ ¼å¼ï¼Œå¦‚xlsxï¼š\n",
    "# df = pd.read_excel('./data/æ—¶é—´.xlsx')\n",
    "with open('data\\patents_zf.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# æ‰“å°å‰ä¸¤æ¡æ•°æ®è¿›è¡Œæ£€æŸ¥\n",
    "for item in data[-2:]:\n",
    "    print(item['year'])\n",
    "    #print(item['title'])\n",
    "    #print(item['abstract'])\n",
    "    print(item['combined_text'])\n",
    "\n",
    "# æ–‡æœ¬\n",
    "timestamps = [item['year'] for item in data]\n",
    "\n",
    "# æå–yearåˆ—å¹¶è½¬æ¢ä¸ºåˆ—è¡¨\n",
    "#timestamps = df[''].astype(int).tolist()\n",
    "\n",
    "print(len(timestamps), timestamps[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(docs, timestamps, global_tuning=False, evolution_tuning=False)\n",
    "fig_topic_time = topic_model.visualize_topics_over_time(topics_over_time,custom_labels=custom_titles)\n",
    "fig_topic_time.write_html(f'topic_overtime_merged{version}.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
