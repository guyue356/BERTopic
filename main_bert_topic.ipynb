{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŸå§‹æ•°æ®åŠ è½½ï¼ˆdocsï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®æ—¶æ£€æµ‹å¤–éƒ¨å‡½æ•°æ›´æ–°\n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 1. è®¾ç½®å›½å†…é•œåƒæº\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "from bertopic import BERTopic\n",
    "import torch # æ–°å¢ï¼šå¯¼å…¥PyTorchï¼ˆsentence_transformersçš„åº•å±‚ï¼‰\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import joblib\n",
    "import time\n",
    "import json\n",
    "import pandas as pd \n",
    "from report_generator import generate_bertopic_report\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰ˆæœ¬æ§åˆ¶\n",
    "version = 'amb3'\n",
    "start_time = 2000\n",
    "end_time = 2025\n",
    "data_source = 'GFä¸“åˆ©'\n",
    "model_name = \"all-mpnet-base-v2\" #çŸ­æ–‡æœ¬\n",
    "# model_name = 'jinaai/jina-embeddings-v2-base-en' #é•¿æ–‡æœ¬,GPUå®¹æ˜“ç‚¸\n",
    "\n",
    "year_range = f'{start_time}-{end_time}'\n",
    "\n",
    "# å…¨é‡æ•°æ®é›†çš„è¯åµŒå…¥ç»“æœä¿å­˜ä½ç½®ï¼Œè®¡ç®—è€—æ—¶é•¿ï¼Œä¸€å®šè¦ä¿å­˜ï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œåªæœ‰æ•°æ®é›†éœ€è¦å¢åŠ æˆ–å˜æ›´æ—¶æ‰éœ€è¦é‡æ–°è®¡ç®—\n",
    "embeddings_path = r'results\\embedding_results\\embeddings_patents_zf_amb_slide_window3.pkl' \n",
    "# HDBSCANç½‘æ ¼æœç´¢èŒƒå›´è®¾ç½®\n",
    "search_sizes = [15, 20, 30, 50, 70,100,150,200]\n",
    "# å½“ä¸»é¢˜æ•°é‡éå¸¸åºå¤§æ—¶ï¼Œå¯ä»¥è¿›ä¸€æ­¥å±‚æ¬¡èšç±»åˆå¹¶ä¸»é¢˜ï¼Œn_groups æ˜¯ä½ æƒ³è¦çš„åˆ†ç»„æ•°é‡\n",
    "n_groups = 50\n",
    "\n",
    "# --- UMAP å‚æ•°è®¾ç½® ---\n",
    "n_neighbors = 15\n",
    "umap_params = {\n",
    "    \"n_neighbors\": n_neighbors,\n",
    "    \"n_components\": 5,\n",
    "    \"min_dist\": 0.0,\n",
    "    \"metric\": 'cosine',\n",
    "    \"random_state\": 5,\n",
    "    \"low_memory\": True\n",
    "}\n",
    "\n",
    "# --- HDBSCANå‚æ•°è®¾ç½® ---\n",
    "HDBSCAN_cfg = {\n",
    "    \"min_samples\": 10,\n",
    "    \"metric\": 'euclidean',\n",
    "    \"prediction_data\": True\n",
    "}\n",
    "\n",
    "# --- vectorizer_model å‚æ•°è®¾ç½® ---\n",
    "vectorizer_params = {\n",
    "    \"ngram_range\": (1, 3), # è¯ç»„èŒƒå›´ï¼š1-3ä¸ªè¯\n",
    "    \"min_df\": 20, # è¿‡æ»¤ä½é¢‘å™ªå£°ï¼Œå¯¹å¤§æ ·æœ¬éå¸¸é‡è¦\n",
    "    \"max_features\": 100000, # é˜²æ­¢å†…å­˜æº¢å‡º\n",
    "    \"max_df\": 0.7 #è¿‡æ»¤æ‰åœ¨è¶…è¿‡ 50% æ–‡æœ¬ä¸­éƒ½å‡ºç°çš„è¯ã€‚è¿™äº›è¯é€šå¸¸æ˜¯è¡Œä¸šèƒŒæ™¯è¯,ä¼šå¹²æ‰°ä¸»é¢˜å…³é”®è¯çš„æå–ã€‚\n",
    "}\n",
    "\n",
    "# --- AIè¯†åˆ«ç³»ç»Ÿæç¤ºè¯è®¾è®¡ ---\n",
    "system_prompt = \"\"\"ä½ æ˜¯ä¸€åä¸“ä¸šçš„å›½é˜²ä¸“åˆ©ä¸æŠ€æœ¯åˆ†æä¸“å®¶ã€‚\n",
    "ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„ä¸»é¡Œå…³é”®è¯ï¼Œä¸ºæ¯ä¸€ä¸ªä¸»é¢˜ç¡®å®šæ¦‚æ‹¬æ€§åç§°ã€‚\n",
    "è¦æ±‚ï¼š\n",
    "1. ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚\n",
    "2. åç§°æ ¼å¼ä¸º\"ç¼–å·ï¼šä¸»é¢˜åç§°\", \n",
    "ç¤ºä¾‹å¦‚ä¸‹\n",
    "0: \"1.æ­¦å™¨è£…å¤‡ç»“æ„è®¾è®¡ä¸åˆ¶å›¾\", \n",
    "1: \"2.åŠå¯¼ä½“å·¥è‰ºä¸å¾®ç”µå­å™¨ä»¶\"\n",
    "\n",
    "3. è¾“å‡ºJSONæ ¼å¼çš„å­—å…¸ï¼Œéµä¸ºä¸»é¢˜ç¼–å·(æ•°æ®ç±»å‹ä¸ºæ•´å‹int)ï¼Œå€¼ä¸ºä¸»é¢˜åç§°(string)ã€‚\n",
    "4. ä¸»é¢˜ç®€æ˜ï¼Œä¸è¦æœ‰å¤šä½™çš„è§£é‡Šã€‚\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»»åŠ¡è¯´æ˜ï¼ˆä»¥æ–‡çŒ®æ•°æ®å¤„ç†ä¸ºä¾‹ï¼‰ï¼š\n",
    "- 1.å°†EXCELæ•°æ®è½¬æ¢ä¸ºjsonæ ¼å¼ï¼ŒæŒ‰ç…§3TI ã€1ABå’Œ2KEY WORDSï¼Œå®Œæˆæ–‡æœ¬çš„å­—ç¬¦ä¸²æ‹¼æ¥ï¼Œjsonæ–‡ä»¶ä¸­éœ€è¦`year`ã€`title`ã€`abstract`ã€`keywords`ä»¥åŠ`combined_text`å­—æ®µï¼›ï¼ˆAIåº”è¯¥å¾ˆå®¹æ˜“å†™å‡ºæ¥ç›¸åº”çš„è½¬æ¢ä»£ç ï¼Œè¯¥æ–‡ä»¶ä½œä¸ºåŸå§‹ä¸»é¢˜å»ºæ¨¡çš„åŸå§‹æ•°æ®æ–‡ä»¶ï¼‰ï¼›\n",
    "- 2.å˜é‡`docs`æ˜¯æ¨¡å‹çš„è¾“å…¥ï¼Œå°†jsonæ–‡ä»¶ä¸­çš„`combined_text`ä¼ å…¥åˆ°è¯¥å‘é‡ä¸­å½¢æˆä¸€ä¸ªåˆ—è¡¨å‹æ•°æ®ï¼›\n",
    "- 3.æ ¹æ®åç»­ä»£ç è·‘é€šè¿™ä¸€å¥—BERTopicæŠ€æœ¯è·¯çº¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®\n",
    "with open('data\\patents_zf.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# # æ‰“å°æœ€åä¸¤æ¡æ•°æ®è¿›è¡Œæ£€æŸ¥\n",
    "# for item in data[-2:]:\n",
    "#     print(item['year'])\n",
    "#     #print(item['title'])\n",
    "#     #print(item['abstract'])\n",
    "#     print(item['combined_text'])\n",
    "\n",
    "# æ–‡æœ¬\n",
    "docs_original = [item['combined_text'] for item in data]\n",
    "timestamps = [item['year'] for item in data]\n",
    "\n",
    "# æ•°æ®æ¸…æ´—\n",
    "def preprocess_all(texts):\n",
    "    cleaned_list = []\n",
    "    for t in texts:\n",
    "        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼\n",
    "        t = re.sub(r\"[^\\w\\s\\.,;]\", \" \", t) # åªä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼å’Œå‡ ç§æ ‡ç‚¹\n",
    "        t = re.sub(r\"\\s+\", \" \", t).strip() # ç§»é™¤å¤šä½™ç©ºæ ¼\n",
    "        cleaned_list.append(t)\n",
    "    return cleaned_list\n",
    "\n",
    "# è¿è¡Œ\n",
    "# æ£€æŸ¥æ¸…æ´—æ•ˆæœ\n",
    "docs = preprocess_all(docs_original)\n",
    "# æ£€æŸ¥æ•°æ®å¯¹é½\n",
    "doc_count = len(docs)\n",
    "time_count = len(timestamps)\n",
    "\n",
    "# å¦‚æœæ•°é‡å¯¹ä¸ä¸Šå°±æŠ¥é”™ï¼Œå¹¶æ‰“å°å‡ºå…·ä½“æ•°å€¼\n",
    "assert doc_count == time_count, f\"æ•°æ®ä¸åŒ¹é…ï¼æ–‡æ¡£æœ‰ {doc_count} æ¡ï¼Œä½†æ—¶é—´æˆ³æœ‰ {time_count} ä¸ªã€‚\"\n",
    "\n",
    "print(f\"æ•°æ®æ£€æŸ¥é€šè¿‡ï¼Œå…± {doc_count} æ¡è®°å½•ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('å…±è·å–æ•°æ®æ€»é‡:', len(docs),'æ¡')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆ›å»ºæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPUåŒæ ·å¯ä»¥è¿è¡Œï¼Œåªæ˜¯é€Ÿåº¦è¾ƒæ…¢\n",
    "# --- æ ¸å¿ƒï¼šæ£€æŸ¥å¹¶æŒ‡å®šè®¾å¤‡ ---\n",
    "# æ£€æŸ¥CUDAï¼ˆå³NVIDIA GPUæ”¯æŒï¼‰æ˜¯å¦å¯ç”¨\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"âœ… GPUå¯ç”¨ï¼Œæ­£åœ¨ä½¿ç”¨: {torch.cuda.get_device_name(0)}\")\n",
    "    # å¯é€‰ï¼šå¦‚æœä½ æƒ³ä½¿ç”¨ç‰¹å®šçš„GPUï¼ˆå¦‚ç¬¬0å—ï¼‰\n",
    "    # device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"âš ï¸  æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œå°†ä½¿ç”¨CPUã€‚\")\n",
    "    import torch\n",
    "print(torch.__version__) # æŸ¥çœ‹PyTorchç‰ˆæœ¬\n",
    "print(torch.cuda.is_available()) # æ ¸å¿ƒï¼šæŸ¥çœ‹CUDAæ˜¯å¦å¯ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# é¢„è®­ç»ƒæ¨¡å‹åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ç»Ÿä¸€å®šä¹‰è·¯å¾„\n",
    "local_model_path = f\"./my_models/{model_name}\"  # å›ºå®šè·¯å¾„\n",
    "\n",
    "# 2. æ£€æŸ¥å¹¶åŠ è½½æ¨¡å‹\n",
    "if not os.path.exists(local_model_path):\n",
    "    print(f\"æ¨¡å‹ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä¸‹è½½å¹¶ä¿å­˜åˆ°æœ¬åœ°: {model_name}\")\n",
    "    # ç›´æ¥ä»huggingfaceä¸‹è½½ambçŸ­æ–‡æœ¬åµŒå…¥æ¨¡å‹\n",
    "    sentence_model = SentenceTransformer(model_name)\n",
    "    # # ç›´æ¥ä»huggingfaceä¸‹è½½jinaié•¿æ–‡æœ¬åµŒå…¥æ¨¡å‹\n",
    "    # sentence_model = SentenceTransformer(model_name,trust_remote_code=True)\n",
    "\n",
    "    # ä¿å­˜åˆ°æœ¬åœ°\n",
    "    sentence_model.save(local_model_path)\n",
    "    print(f\"æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{local_model_path}\")\n",
    "else:\n",
    "    print(f\"æ¨¡å‹å·²å­˜åœ¨ï¼Œä»æœ¬åœ°åŠ è½½: {local_model_path}\")\n",
    "    # ä»æœ¬åœ°åŠ è½½\n",
    "    sentence_model = SentenceTransformer(local_model_path)\n",
    "    #sentence_model = SentenceTransformer(local_model_path,trust_remote_code=True)\n",
    "# sentence_model.max_seq_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slide_window import sliding_window_encode\n",
    "# 2. è®¡ç®—å¹¶ä¿å­˜è¯å‘é‡\n",
    "def compute_and_save_embeddings(docs, save_path='embeddings_test'):\n",
    "    \"\"\"è®¡ç®—å¹¶ä¿å­˜è¯å‘é‡\"\"\"\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"åŠ è½½å·²ä¿å­˜çš„åµŒå…¥: {save_path}\")\n",
    "        with open(save_path, 'rb') as f:\n",
    "            embeddings = pickle.load(f)\n",
    "    else:\n",
    "        print(\"è®¡ç®—æ–°çš„åµŒå…¥...\")\n",
    "        embeddings = sliding_window_encode(\n",
    "            docs, \n",
    "            sentence_model, \n",
    "            window_size=512,\n",
    "            stride=256,\n",
    "            inference_batch_size=32\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        # embeddings = sentence_model.encode(docs,\n",
    "        #                                 batch_size=2,\n",
    "        #                                 show_progress_bar=True,\n",
    "        #                                 convert_to_numpy=True # æ˜¾å¼è½¬ä¸º numpy é‡Šæ”¾ GPU ç©ºé—´\n",
    "        #                                 )\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "        print(f\"åµŒå…¥å·²ä¿å­˜åˆ°: {save_path}\")\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file_path):\n",
    "    \"\"\"ä»JSONæ–‡ä»¶åŠ è½½åœç”¨è¯åˆ—è¡¨\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            stopwords = json.load(f)\n",
    "        print(f\"å·²åŠ è½½ {len(stopwords['common'])} ä¸ªåœç”¨è¯\")\n",
    "        return stopwords['common']  # è½¬æ¢ä¸ºé›†åˆæé«˜æŸ¥æ‰¾æ•ˆç‡\n",
    "    except FileNotFoundError:\n",
    "        print(f\"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨\")\n",
    "        return set()\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼\")\n",
    "        return set()\n",
    "\n",
    "# # æµ‹è¯•ç¤ºä¾‹\n",
    "# stopwords = load_stopwords('data\\stopwords.json')\n",
    "# print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š BERTopic æ ¸å¿ƒæ¨¡å‹å‚æ•°è®¾ç½®æŒ‡å—\n",
    "\n",
    "åœ¨ BERTopic ä¸­ï¼Œ**UMAPï¼ˆé™ç»´ï¼‰** å’Œ **HDBSCANï¼ˆèšç±»ï¼‰** æ˜¯å†³å®šä¸»é¢˜è´¨é‡çš„ä¸¤ä¸ªå…³é”®ç¯èŠ‚ã€‚å¯¹äº 12.5 ä¸‡æ¡çš„å¤§è§„æ¨¡é•¿æ–‡æœ¬æ•°æ®ï¼Œåˆç†çš„å‚æ•°é…ç½®èƒ½å¹³è¡¡â€œç»†èŠ‚æ•æ‰â€ä¸â€œç»“æ„ç¨³å¥æ€§â€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸€ã€ UMAPï¼šå°†é«˜ç»´è¯­ä¹‰å‹ç¼©è‡³ä½ç»´\n",
    "\n",
    "**ä¸»è¦ä»»åŠ¡ï¼š** æŠŠ BERT æå–çš„å‡ ç™¾ç»´å‘é‡é™è‡³ 5 ç»´ï¼ŒåŒæ—¶ä¿ç•™æ–‡æœ¬é—´çš„ç›¸ä¼¼æ€§ã€‚\n",
    "\n",
    "| å‚æ•°åç§° | è®¾ç½®å»ºè®® | é€šä¿—ç†è§£ |\n",
    "| --- | --- | --- |\n",
    "| **`n_neighbors`** | **20** | **è§†é‡èŒƒå›´**ã€‚å†³å®šæ¯ä¸ªç‚¹çœ‹å¤šè¿œã€‚å€¼è®¾ä¸º 20 èƒ½åœ¨ä¿ç•™å±€éƒ¨ç»†èŠ‚çš„åŒæ—¶ï¼Œå…¼é¡¾ 12.5 ä¸‡æ¡æ•°æ®çš„æ•´ä½“åˆ†å¸ƒï¼Œé˜²æ­¢ä¸»é¢˜è¿‡äºç ´ç¢ã€‚ |\n",
    "| **`n_components`** | **5** | **ä¿ç•™ç»´åº¦**ã€‚é™ç»´åçš„â€œç»´åº¦æ•°â€ã€‚è®¾ç½®ä¸º 5 æ¯” 2 æˆ– 3 èƒ½ä¿ç•™æ›´å¤šè¯­ä¹‰ç»†èŠ‚ï¼Œé¿å…ä¿¡æ¯åœ¨å‹ç¼©è¿‡ç¨‹ä¸­ä¸¢å¤±ã€‚ |\n",
    "| **`min_dist`** | **0.0** | **æŒ¤å‹ç¨‹åº¦**ã€‚ç‚¹ä¸ç‚¹ä¹‹é—´çš„æœ€å°é—´è·ã€‚è®¾ç½®ä¸º 0.0 ä¼šè®©ç›¸ä¼¼çš„æ–‡æ¡£å°½å¯èƒ½â€œæŠ±å›¢â€ï¼Œæœ‰åŠ©äº HDBSCAN å‘ç°æ›´æ¸…æ™°çš„è¾¹ç•Œã€‚ |\n",
    "| **`metric`** | **'cosine'** | **ç›¸ä¼¼åº¦æ ‡å‡†**ã€‚æ–‡æœ¬å¤„ç†çš„é»„é‡‘æ ‡å‡†ã€‚å®ƒå…³æ³¨çš„æ˜¯è¯æ±‡çš„æ–¹å‘ï¼ˆè¯­ä¹‰ï¼‰ï¼Œè€Œä¸æ˜¯æ–‡æœ¬çš„é•¿çŸ­ã€‚ |\n",
    "| **`random_state`** | **5** (æˆ– 42) | **ç»“æœä¿é™©å•**ã€‚å›ºå®šéšæœºç§å­ã€‚ç¡®ä¿ä½ ä¸‹æ¬¡è¿è¡Œä»£ç æ—¶ï¼Œé™ç»´åçš„ç»“æœæ˜¯ä¸€æ¨¡ä¸€æ ·çš„ã€‚ |\n",
    "\n",
    "---\n",
    "\n",
    "## äºŒã€ HDBSCANï¼šåœ¨ä½ç»´ç©ºé—´å‘ç°ä¸»é¢˜\n",
    "\n",
    "**ä¸»è¦ä»»åŠ¡ï¼š** åœ¨é™ç»´åçš„ç©ºé—´é‡Œå¯»æ‰¾é«˜å¯†åº¦åŒºåŸŸï¼Œæ¯ä¸ªé«˜å¯†åº¦åŒºåŸŸå°±æ˜¯ä¸€ä¸ªâ€œä¸»é¢˜â€ã€‚\n",
    "\n",
    "| å‚æ•°åç§° | è®¾ç½®å»ºè®® | é€šä¿—ç†è§£ |\n",
    "| --- | --- | --- |\n",
    "| **`min_cluster_size`** | **50** | **æˆå›¢é—¨æ§›**ã€‚å®šä¹‰ä¸€ä¸ªä¸»é¢˜æœ€å°‘è¦åŒ…å«å¤šå°‘ç¯‡æ–‡æ¡£ã€‚å¯¹äº 12.5 ä¸‡æ¡æ•°æ®ï¼Œ50 æ˜¯ä¸€ä¸ªè¾ƒç»†çš„ç²’åº¦ï¼›å¦‚æœè§‰å¾—ä¸»é¢˜å¤ªç¢ï¼Œå¯ä»¥è°ƒå¤§è‡³ 200-500ã€‚ |\n",
    "| **`min_samples`** | **3** | **ä¸¥è‹›ç¨‹åº¦**ã€‚å®šä¹‰æ ¸å¿ƒç‚¹çš„é—¨æ§›ã€‚å€¼è¶Šå°ï¼ˆå¦‚ 3ï¼‰ï¼Œç®—æ³•è¶Šâ€œå®½å®¹â€ï¼Œä¼šå°†æ›´å¤šè¾¹ç¼˜ç‚¹çº³å…¥ä¸»é¢˜ï¼›å€¼è¶Šå¤§ï¼Œå™ªå£°ï¼ˆ-1ï¼‰ä¼šè¶Šå¤šã€‚ |\n",
    "| **`metric`** | **'euclidean'** | **æµ‹è·æ–¹å¼**ã€‚ç”±äº UMAP å·²ç»å¤„ç†è¿‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œé™ç»´åçš„ 5 ç»´ç©ºé—´é€šå¸¸ç›´æ¥ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»æ¥åˆ’åˆ†ç°‡ã€‚ |\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸‰ã€ é’ˆå¯¹â€œé™å™ªâ€ä¸â€œç²’åº¦â€çš„è°ƒä¼˜ç§˜ç±\n",
    "\n",
    "å¦‚æœä½ åœ¨è¿è¡Œåå‘ç°æ•ˆæœä¸ç†æƒ³ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹é€»è¾‘è¿›è¡Œå¾®è°ƒï¼š\n",
    "\n",
    "### 1. å™ªå£°ï¼ˆ-1ï¼‰æ¯”ä¾‹å¤ªé«˜æ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "* **æ–¹æ³• Aï¼ˆè°ƒæ¾ï¼‰ï¼š** ä¿æŒ `min_samples=3` ç”šè‡³é™ä¸º **1**ã€‚è¿™ä¼šå¼ºåˆ¶è®©æ›´å¤šæ¸¸ç¦»çš„æ–‡æ¡£å½’å…¥æœ€è¿‘çš„ä¸»é¢˜ã€‚\n",
    "* **æ–¹æ³• Bï¼ˆåå¤„ç†ï¼‰ï¼š** ä¸åŠ¨èšç±»æ¨¡å‹ï¼Œè®­ç»ƒåä½¿ç”¨ `topic_model.reduce_outliers()` å¼ºåˆ¶å½’ç±»ã€‚\n",
    "\n",
    "### 2. ä¸»é¢˜å¤ªç»†ç¢ã€æ•°é‡å¤ªå¤šæ€ä¹ˆåŠï¼Ÿ\n",
    "\n",
    "* **å¢åŠ  `min_cluster_size**`ï¼šä» 50 æé«˜åˆ° **300 æˆ– 500**ã€‚è¿™æ˜¯æ§åˆ¶ä¸»é¢˜æ€»æ•°æœ€ç›´æ¥çš„æ æ†ã€‚\n",
    "* **åæœŸåˆå¹¶**ï¼šåˆ©ç”¨ `topic_model.reduce_topics(nr_topics=100)` æ‰‹åŠ¨æŒ‡å®šæœ€ç»ˆæƒ³è¦çš„ä¸»é¢˜æ•°ã€‚\n",
    "\n",
    "### 3. ä¸»é¢˜ä¹‹é—´åˆ†ä¸å¼€ï¼ˆè¯­ä¹‰é‡å ï¼‰ï¼Ÿ\n",
    "\n",
    "* **è°ƒå° `n_neighbors**`ï¼šä» 20 é™åˆ° **15 æˆ– 10**ã€‚è¿™ä¼šè¿«ä½¿ UMAP æ›´åŠ å…³æ³¨å±€éƒ¨å·®å¼‚ï¼Œè®©ä¸åŒçš„ç°‡åˆ†å¾—æ›´å¼€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "> **ğŸ’¡ æç¤ºï¼š**\n",
    "> å¯¹äºå¤§è§„æ¨¡æ•°æ®ï¼Œå»ºè®®å…ˆç”¨è¿™å¥—å‚æ•°è·‘ä¸€ä¸ªæ ·æœ¬é›†ï¼ˆå¦‚ 2 ä¸‡æ¡ï¼‰ï¼Œè§‚å¯Ÿä¸»é¢˜åˆ†å¸ƒã€‚ä¸€æ—¦ç¡®å®š UMAP æ•ˆæœä¸é”™ï¼Œè¯·åŠ¡å¿…**ä¿å­˜ UMAP ç»“æœ**ï¼ˆ`.npy` æˆ– `.joblib`ï¼‰ï¼Œä»¥èŠ‚çœåç»­åå¤è°ƒå‚çš„æ—¶é—´æˆæœ¬ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopicæ¨¡å‹åˆ›å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½ çš„ä»£ç æµç¨‹ï¼š\n",
    "# +-------------------+       +-------------------------+       +---------------+\n",
    "# | ä½ çš„é¢„è®¡ç®—åµŒå…¥     |------>| BERTopic.fit_transform()|------>|æœ€ç»ˆä¸»é¢˜ç»“æœ   |\n",
    "# | embeddings_patents|       |                         |       |               |\n",
    "# | _zf_v1.pkl        |       |                         |       | topics, probs |\n",
    "# +-------------------+       +-------------------------+       +---------------+\n",
    "#                                    â”‚\n",
    "#                                    â–¼ å†…éƒ¨å¤„ç†æµç¨‹\n",
    "#                         +-----------------------+\n",
    "#                         â”‚ 1. æ¥æ”¶ä½ çš„ embeddings â”‚\n",
    "#                         +-----------------------+\n",
    "#                                    â”‚\n",
    "#                                    â–¼\n",
    "#                         +-----------------------+\n",
    "#                         â”‚ 2. âœ… UMAP é™ç»´        â”‚\n",
    "#                         +-----------------------+\n",
    "#                                    â”‚\n",
    "#                                    â†“\n",
    "#                         +-----------------------+\n",
    "#                         â”‚ 3. HDBSCAN èšç±»        â”‚\n",
    "#                         +-----------------------+\n",
    "#                                    â”‚\n",
    "#                                    â†“\n",
    "#                         +----------------------------------+\n",
    "#                         â”‚ 4. Vectorizer\\c-TF-IDFæå–ä¸»é¢˜è¯ç­‰â”‚\n",
    "#                         +----------------------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¯å‘é‡åµŒå…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¡ç®—å¹¶ä¿å­˜è¯å‘é‡ï¼Œå…¨é‡è¯å‘é‡åµŒå…¥ï¼Œå»ºè®®ä¿å­˜ï¼Œä»¥å…é‡å¤è®¡ç®—\n",
    "embeddings = compute_and_save_embeddings(docs, embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAPé™ç»´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–‡ä»¶è·¯å¾„å®šä¹‰\n",
    "umap_cache_path = f'umap_model_{data_source}_{start_time}-{end_time}_{version}.joblib'\n",
    "umap_model = UMAP(**umap_params)\n",
    "# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨\n",
    "if os.path.exists(umap_cache_path):\n",
    "    print(f\"ğŸš€ æ£€æµ‹åˆ°å·²å­˜åœ¨çš„é™ç»´æ¨¡å‹ï¼Œæ­£åœ¨ç›´æ¥åŠ è½½: {umap_cache_path}\")\n",
    "    umap_embeddings = joblib.load(umap_cache_path)\n",
    "else:\n",
    "    print(\"â³ æœªå‘ç°é™ç»´æ¨¡å‹ï¼Œå¼€å§‹æ‰§è¡Œ UMAP é™ç»´è®¡ç®—ï¼ˆå¤§æ•°æ®é‡è€—æ—¶è¾ƒé•¿ï¼‰...\")\n",
    "    # ç¡®ä¿ umap_params å·²ç»å®šä¹‰\n",
    "    umap_embeddings = umap_model.fit_transform(embeddings)\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    joblib.dump(umap_embeddings, umap_cache_path)\n",
    "    print(f\"âœ” UMAP é™ç»´å®Œæˆå¹¶å·²ä¿å­˜è‡³: {umap_cache_path}\")\n",
    "\n",
    "# æ¥ä¸‹æ¥å¯ä»¥ç›´æ¥ä½¿ç”¨ umap_embeddings è¿›è¡Œ HDBSCAN èšç±»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCANèšç±»ï¼ˆç½‘æ ¼æœç´¢ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_topics = None\n",
    "best_m_size = None\n",
    "best_score = float('inf')\n",
    "\n",
    "search_history = [] # æ–°å¢ï¼šç”¨äºè®°å½•ç½‘æ ¼æœç´¢è¿‡ç¨‹\n",
    "\n",
    "print(f\"{'Size':<10} | {'ä¸»é¢˜æ•°':<8} | {'è´Ÿæ ·æœ¬æ•°':<10} | {'å™ªå£°æ¯”ä¾‹':<10} | {'è€—æ—¶':<8}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for m_size in search_sizes:\n",
    "    start_t = time.time()\n",
    "  \n",
    "    clusterer = HDBSCAN(**HDBSCAN_cfg,min_cluster_size=m_size)\n",
    "\n",
    "    \n",
    "    labels = clusterer.fit_predict(umap_embeddings)\n",
    "    \n",
    "    # è®¡ç®—æŒ‡æ ‡\n",
    "    n_outliers = (labels == -1).sum() # è´Ÿæ ·æœ¬æ•°\n",
    "    n_topics = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    outlier_perc = n_outliers / len(labels)\n",
    "    duration = time.time() - start_t\n",
    "\n",
    "    # ä¿å­˜å†å²è®°å½•\n",
    "    res = {\n",
    "        \"min_cluster_size\": m_size,\n",
    "        \"n_topics\": n_topics,\n",
    "        \"n_outliers\": n_outliers,\n",
    "        \"outlier_perc\": f\"{outlier_perc:.1%}\",\n",
    "        \"duration\": f\"{duration:.1f}s\",\n",
    "        \"is_best\": False\n",
    "    }\n",
    "    search_history.append(res)\n",
    "\n",
    "    # ç­›é€‰é€»è¾‘ï¼šé€‰å™ªå£°æœ€å°çš„\n",
    "    if outlier_perc < best_score:\n",
    "        best_topics = n_topics\n",
    "        best_score = outlier_perc\n",
    "        best_m_size = m_size\n",
    "    \n",
    "\n",
    "    print(f\"{m_size:<10} | {n_topics:<10} | {n_outliers:<12} | {outlier_perc:<12.1%} | {duration:<8.1f}s\")\n",
    "print(\"-\" * 65)\n",
    "if best_m_size is None:\n",
    "    print(\"æœªåœ¨é¢„è®¾ä¸»é¢˜æ•°èŒƒå›´å†…æ‰¾åˆ°å‚æ•°ï¼Œå»ºè®®è°ƒä½ min_dist æˆ–æ£€æŸ¥ UMAP æ•ˆæœ\")\n",
    "else:\n",
    "    print(f\"ğŸ† ç¬¦åˆæ¡ä»¶çš„è´Ÿæ ·æœ¬æ•°æœ€å°‘çš„å‚æ•°å€¼: min_cluster_size = {best_m_size} (è´Ÿæ ·æœ¬æ¯”ä¾‹: {best_score:.1%},ä¸»é¢˜æ•°ï¼š{best_topics})\")\n",
    "\n",
    "# æ ‡è®°æœ€ä¼˜å‚æ•°\n",
    "for item in search_history:\n",
    "    if item[\"min_cluster_size\"] == best_m_size:\n",
    "        item[\"is_best\"] = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­£å¼åˆ›å»ºBERTopicæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HDBSCANèšç±»\n",
    "# ä½¿ç”¨ ** è¿›è¡Œå­—å…¸è§£åŒ…\n",
    "best_clusterer = HDBSCAN(**HDBSCAN_cfg,min_cluster_size=best_m_size)\n",
    "# åŠ è½½åœç”¨è¯\n",
    "stop_words = load_stopwords('data\\stopwords.json')\n",
    "# åˆ›å»ºCountVectorizeræ¨¡å‹,è‡ªå®šä¹‰åœç”¨è¯\n",
    "# vectorizer_model = CountVectorizer(\n",
    "#     ngram_range = (1,3), # è¯ç»„èŒƒå›´ï¼š1-3ä¸ªè¯\n",
    "#     stop_words = stop_words, # åœç”¨è¯\n",
    "#     min_df = 10, # è¿‡æ»¤ä½é¢‘å™ªå£°ï¼Œå¯¹å¤§æ ·æœ¬éå¸¸é‡è¦\n",
    "#     max_features=100000 # é˜²æ­¢å†…å­˜æº¢å‡º\n",
    "#     )\n",
    "vectorizer_model = CountVectorizer(\n",
    "    **vectorizer_params,\n",
    "    stop_words = stop_words\n",
    "    )\n",
    "\n",
    "# åˆå§‹åŒ– BERTopic æ—¶å¸¦ä¸Šå®ƒ\n",
    "topic_model = BERTopic(\n",
    "  embedding_model=sentence_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  umap_model=umap_model,       # åŠ ä¸Šè¿™ä¸€è¡Œ\n",
    "  hdbscan_model=best_clusterer # åŠ ä¸Šè¿™ä¸€è¡Œ\n",
    ")\n",
    "#ä¼ å…¥è®­ç»ƒå¥½çš„è¯å‘é‡ï¼Œfit_transformer()åŒ…æ‹¬äº†UMAPé™ç»´+HDBSCANèšç±»ï¼Œä¸å»ºè®®è¿™ä¹ˆåšï¼Œå¯ä»¥å°†å…¶æ‹†å¼€å‡å°‘è®¡ç®—\n",
    "#topics, probs = topic_model.fit_transform(docs, embeddings=embeddings,y = labels) \n",
    "#probè®¡ç®—å¾ˆèŠ±æ—¶é—´\n",
    "topics, _ = topic_model.fit_transform(docs, embeddings=embeddings)\n",
    "# ä¿å­˜æ•´ä¸ªä¸»é¢˜æ¨¡å‹\n",
    "\n",
    "\n",
    "save_path = rf\"results\\topic_models\\bertopic_{data_source}_{year_range}_{version}\"\n",
    "topic_model.save(save_path)\n",
    "print(rf\"ä¸»é¢˜æ¨¡å‹å·²ä¿å­˜åˆ° results\\topic_models\\bertopic_{data_source}_{year_range}_{version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ç›´æ¥åŠ è½½ä¸»é¢˜æ¨¡å‹\n",
    "topic_model = BERTopic.load(rf\"results\\topic_models\\bertopic_{data_source}_{year_range}_{version}\")\n",
    "print(\"åˆå§‹ä¸»é¢˜æ¨¡å‹åŠ è½½æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŸå§‹ä¸»é¢˜å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é™è‡³äºŒç»´ï¼Œå†å¯è§†åŒ–\n",
    "reduced_embeddings = UMAP(n_neighbors=n_neighbors, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 0. åŸºç¡€è®¾ç½® ---\n",
    "title_base = f'{year_range}å¹´{data_source}æ•°æ®Bertopicä¸»é¢˜èšç±»ç»“æœä¸€è§ˆè¡¨-{version}'\n",
    "output_dir = f\"BERTopic_Results_Allset_{year_range}_{data_source}_{version}\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# --- å‚æ•°è¯´æ˜ ---\n",
    "report_name = os.path.join(output_dir, f\"report_{year_range}_{data_source}_{version}.html\")\n",
    "generate_bertopic_report(umap_cfg = umap_params, \n",
    "                         HDBSCAN_cfg=HDBSCAN_cfg,\n",
    "                         vectorizer_cfg = vectorizer_params,\n",
    "                         history= search_history,\n",
    "                         best_size = best_m_size,\n",
    "                         model_name = model_name, \n",
    "                         output_path = report_name)\n",
    "print(f\"âœ” å‚æ•°è¯´æ˜æ–‡æ¡£å·²ç”Ÿæˆï¼š{report_name}\")\n",
    "\n",
    "# --- 1. ç”Ÿæˆå›¾è¡¨ (æŒ‰ç…§ä½ æƒ³è¦çš„é¡ºåº) ---\n",
    "#-------------------------------------------------------------------------\n",
    "# [å›¾ A] ä¸»é¢˜å…³é”®è¯æ¡å½¢å›¾ - å­˜ä¸º barchart.html\n",
    "fig_barchart = topic_model.visualize_barchart(\n",
    "    top_n_topics=50, # åªé¢„è§ˆå‰50ä¸ªä¸»é¢˜ï¼Œè‹¥éœ€è¦æ‰€æœ‰ä¸»é¢˜çš„å…³é”®è¯ï¼Œåˆ™è®¾ç½®ä¸ºbest_topics\n",
    "    custom_labels=True,\n",
    "    n_words=10,\n",
    "    height=400,\n",
    "    title=f\"{year_range}å¹´{data_source}å„ä¸»é¢˜å…³é”®è¯çš„c-TF-IDFæƒé‡å¾—åˆ†æ¡å½¢å›¾\",\n",
    ")\n",
    "\n",
    "fig_barchart.update_layout(\n",
    "    # å› ä¸ºæ¯è¡Œå¢åŠ äº†å­å›¾ï¼Œå»ºè®®å¢åŠ æ€»å®½åº¦ä»¥é˜²é‡å \n",
    "    width=1500, \n",
    "    # ç»Ÿä¸€å­—ä½“æ ·å¼\n",
    "    font=dict(family=\"KaiTi\", size=16),\n",
    "    title_font=dict(family=\"KaiTi\", size=36, color=\"black\", weight=\"bold\")\n",
    ")\n",
    "path_barchart = os.path.join(output_dir, \"barchart.html\")\n",
    "fig_barchart.write_html(path_barchart)\n",
    "#-------------------------------------------------------------------------\n",
    "# [å›¾ B] å±‚æ¬¡èšç±»å›¾ - å­˜ä¸º hierarchy.html\n",
    "fig_hierarchy = topic_model.visualize_hierarchy(\n",
    "    hierarchical_topics=hierarchical_topics,\n",
    "    custom_labels=True,\n",
    "    title=f\"{year_range}å¹´{data_source}ä¸»é¢˜å±‚æ¬¡èšç±»å›¾\",\n",
    "    height=800\n",
    ")\n",
    "fig_hierarchy.update_layout(\n",
    "    title_x=0.5,\n",
    "    # å› ä¸ºæ¯è¡Œå¢åŠ äº†å­å›¾ï¼Œå»ºè®®å¢åŠ æ€»å®½åº¦ä»¥é˜²é‡å \n",
    "    width=1500, \n",
    "    # ç»Ÿä¸€å­—ä½“æ ·å¼\n",
    "    font=dict(family=\"KaiTi\", size=16),\n",
    "    title_font=dict(family=\"KaiTi\", size=30, color=\"black\", weight=\"bold\")\n",
    ")\n",
    "path_hierarchy = os.path.join(output_dir, \"hierarchy.html\")\n",
    "fig_hierarchy.write_html(path_hierarchy)\n",
    "#-------------------------------------------------------------------------\n",
    "# [å›¾ C] æ–‡æ¡£åˆ†å¸ƒæ•£ç‚¹å›¾ - å­˜ä¸º documents.html\n",
    "fig_documents = topic_model.visualize_documents(\n",
    "    docs=[doc[:150] + \"...\" for doc in docs],\n",
    "    reduced_embeddings=reduced_embeddings,\n",
    "    custom_labels=True,\n",
    "    hide_document_hover=False\n",
    ")\n",
    "# æ›´æ–°æ•£ç‚¹å›¾æ ·å¼\n",
    "fig_documents.update_layout(\n",
    "    title=f\"{year_range}å¹´{data_source}ä¸»é¢˜åˆ†å¸ƒå›¾\",\n",
    "    title_x=0.5,\n",
    "    width=1500,\n",
    "    height=1200,\n",
    "    margin=dict(l=80, r=80, t=100, b=80),\n",
    "    font=dict(family=\"KaiTi\", size=16, color=\"black\"),\n",
    "    title_font=dict(family=\"KaiTi\", size=30, color=\"black\", weight=\"bold\")\n",
    ")\n",
    "path_documents = os.path.join(output_dir, \"documents.html\")\n",
    "fig_documents.write_html(path_documents)\n",
    "#-------------------------------------------------------------------------\n",
    "# [å›¾ D] ä¸»é¢˜æ—¶åºå›¾ - å­˜ä¸º hierarchy.html\n",
    "\n",
    "\n",
    "\n",
    "topics_over_time = topic_model.topics_over_time(docs, timestamps, global_tuning=False, evolution_tuning=False)\n",
    "fig_topic_time = topic_model.visualize_topics_over_time(\n",
    "    topics_over_time,\n",
    "    custom_labels=True)\n",
    "# æ›´æ–°æ•£ç‚¹å›¾æ ·å¼\n",
    "fig_topic_time.update_layout(\n",
    "    title=f\"{year_range}å¹´{data_source}ä¸»é¢˜æ—¶åºå›¾\",\n",
    "    title_x=0.5,\n",
    "    width=1500,\n",
    "    height=800,\n",
    "    font=dict(family=\"KaiTi\", size=16, color=\"black\"),\n",
    "    title_font=dict(family=\"KaiTi\", size=30, color=\"black\", weight=\"bold\")\n",
    ")\n",
    "path_topic_time = os.path.join(output_dir, \"topic_overtime_merged.html\")\n",
    "fig_topic_time.write_html(path_topic_time)\n",
    "# --- 2. åˆ›å»ºå¯¼èˆªç´¢å¼•é¡µ (index.html) ---\n",
    "# æ³¨æ„ï¼šæˆ‘åœ¨è¿™é‡ŒåŒæ­¥è°ƒæ•´äº†æŒ‰é’®é¡ºåºå’Œ iframe çš„é»˜è®¤ src\n",
    "index_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"zh-CN\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>{title_base}</title>\n",
    "    <style>\n",
    "        body {{ font-family: 'Microsoft YaHei', sans-serif; margin: 0; display: flex; flex-direction: column; height: 100vh; background-color: #f4f7f6; }}\n",
    "        header {{ background: #2c3e50; color: white; padding: 15px 25px; display: flex; justify-content: space-between; align-items: center; box-shadow: 0 2px 5px rgba(0,0,0,0.2); }}\n",
    "        h1 {{ margin: 0; font-size: 20px; }}\n",
    "        nav {{ background: #ecf0f1; padding: 10px; display: flex; gap: 10px; border-bottom: 1px solid #ddd; }}\n",
    "        .nav-btn {{ \n",
    "            padding: 8px 15px; background: white; border: 1px solid #bdc3c7; border-radius: 4px; \n",
    "            cursor: pointer; text-decoration: none; color: #34495e; font-size: 14px; transition: all 0.3s;\n",
    "        }}\n",
    "        .nav-btn:hover {{ background: #3498db; color: white; border-color: #2980b9; }}\n",
    "        .nav-btn.active {{ background: #3498db; color: white; }}\n",
    "        #content-frame {{ flex-grow: 1; border: none; width: 100%; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <header>\n",
    "        <h1>{title_base}</h1>\n",
    "        <span style=\"font-size: 12px; opacity: 0.8;\">ç‰ˆæœ¬: {version}</span>\n",
    "    </header>\n",
    "    \n",
    "    <nav>\n",
    "        <a class=\"nav-btn\" href=\"report_{year_range}_{data_source}_{version}.html\" target=\"chart_frame\">âœ¨ {year_range}å¹´{data_source}ä¸»é¢˜èšç±»å‚æ•°è¯´æ˜</a>\n",
    "        <a class=\"nav-btn\" href=\"barchart.html\" target=\"chart_frame\">ğŸ“ˆ  {year_range}å¹´{data_source}ä¸»é¢˜å…³é”®è¯æƒé‡å›¾</a>\n",
    "        <a class=\"nav-btn\" href=\"hierarchy.html\" target=\"chart_frame\">ğŸ“Š  {year_range}å¹´{data_source}ä¸»é¢˜å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾</a>\n",
    "        <a class=\"nav-btn\" href=\"documents.html\" target=\"chart_frame\">ğŸ“  {year_range}å¹´{data_source}ä¸»é¢˜åˆ†å¸ƒæ•£ç‚¹å›¾</a>\n",
    "        <a class=\"nav-btn\" href=\"topic_overtime_merged.html\" target=\"chart_frame\">âŒš {year_range}å¹´{data_source}ä¸»é¢˜æ—¶åºå›¾</a>\n",
    "    \n",
    "    </nav>\n",
    "\n",
    "    <iframe name=\"chart_frame\" id=\"content-frame\" src=\"report_{year_range}_{data_source}_{version}.html\"></iframe>\n",
    "\n",
    "    <script>\n",
    "        const buttons = document.querySelectorAll('.nav-btn');\n",
    "        buttons.forEach(btn => {{\n",
    "            btn.addEventListener('click', function() {{\n",
    "                buttons.forEach(b => b.classList.remove('active'));\n",
    "                this.classList.add('active');\n",
    "            }});\n",
    "        }});\n",
    "        // é»˜è®¤é«˜äº®ç¬¬ä¸€ä¸ªæŒ‰é’®ï¼ˆå³å‚æ•°è¯´æ˜é¡µé¢ï¼‰\n",
    "        buttons[0].classList.add('active');\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(output_dir, f\"index_{year_range}_{data_source}_res.html\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(index_content)\n",
    "\n",
    "print(f\"ğŸ‰ ç»“æœå¯è§†åŒ–å·²å®Œæˆï¼å·²è¯·æ‰“å¼€æ–‡ä»¶å¤¹: {output_dir}æŸ¥çœ‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å±‚æ¬¡èšç±»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è®¡ç®—å¯ä»¥è¿›ä¸€æ­¥åˆå¹¶çš„ä¸»é¢˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import fcluster, cut_tree\n",
    "\n",
    "# 1. é’ˆå¯¹æ•°æ®ç»“æ„æå– Scipy é“¾æ¥çŸ©é˜µ\n",
    "def get_linkage_matrix_final(df):\n",
    "    # æå–å·¦å­èŠ‚ç‚¹ã€å³å­èŠ‚ç‚¹ã€è·ç¦»\n",
    "    # ä½ çš„åˆ—åæ˜¯ Child_Left_ID, Child_Right_ID, Distance\n",
    "    left = df['Child_Left_ID'].values.astype(float)\n",
    "    right = df['Child_Right_ID'].values.astype(float)\n",
    "    dist = df['Distance'].values.astype(float)\n",
    "    \n",
    "    # æå–ä¸»é¢˜æ•°é‡ï¼šç”±äºä½ çš„ Topics åˆ—å­˜çš„æ˜¯åˆ—è¡¨ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æ¯ä¸ªåˆ—è¡¨çš„é•¿åº¦\n",
    "    num_topics = df['Topics'].apply(len).values.astype(float)\n",
    "    \n",
    "    # åˆå¹¶ä¸º Scipy éœ€è¦çš„ Linkage Matrix (n-1, 4)\n",
    "    return np.column_stack([left, right, dist, num_topics])\n",
    "\n",
    "# 2. æ‰§è¡ŒçŸ©é˜µè½¬æ¢\n",
    "# æ³¨æ„ï¼šScipy è¦æ±‚çŸ©é˜µæŒ‰è·ç¦»ä»å°åˆ°å¤§æ’åºï¼ŒBERTopic è¿”å›çš„é¡ºåºé€šå¸¸æ˜¯åçš„ï¼ˆæ ¹èŠ‚ç‚¹åœ¨æœ€å‰ï¼‰\n",
    "df_sorted = hierarchical_topics.sort_values('Distance')\n",
    "linkage_matrix = get_linkage_matrix_final(df_sorted)\n",
    "\n",
    "# 3. è¿›è¡Œåˆ†ç»„\n",
    "\n",
    "clusters = cut_tree(linkage_matrix, n_clusters=n_groups).flatten()\n",
    "\n",
    "# 4. æ˜ å°„å›åŸå§‹ä¸»é¢˜æ ‡ç­¾\n",
    "# è·å–æ‰€æœ‰åŸå§‹ä¸»é¢˜ IDï¼ˆæ’å¥½åºï¼Œç¡®ä¿ä¸ cut_tree ç»“æœå¯¹åº”ï¼‰\n",
    "all_topics = sorted([t for t in topic_model.get_topics().keys() if t != -1])\n",
    "\n",
    "df_res = pd.DataFrame({\n",
    "    \"topic_id\": all_topics,\n",
    "    \"group\": clusters\n",
    "})\n",
    "\n",
    "# 5. åŠ ä¸Šè¯­ä¹‰æ ‡ç­¾æ–¹ä¾¿æŸ¥çœ‹\n",
    "topic_labels = topic_model.generate_topic_labels(nr_words=3, separator=\"_\")\n",
    "id_to_label = {int(l.split(\"_\")[0]): l for l in topic_labels if not l.startswith(\"-1\")}\n",
    "df_res['label'] = df_res['topic_id'].map(id_to_label)\n",
    "\n",
    "# æŸ¥çœ‹æœ€ç»ˆç»“æœ\n",
    "print(df_res.head(50))\n",
    "\n",
    "# æ£€æŸ¥æ¯ä¸ªç»„åŒ…å«çš„ä¸»é¢˜æ•°\n",
    "print(\"\\næ¯ç»„ä¸»é¢˜æ•°é‡åˆ†å¸ƒï¼š\")\n",
    "print(df_res['group'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. æŒ‰ç…§ group åˆ†ç»„ï¼Œå¹¶å°† topic_id èšåˆä¸ºåˆ—è¡¨\n",
    "grouped_topics = df_res.groupby('group')['topic_id'].apply(list).reset_index()\n",
    "\n",
    "# 2. æ ¼å¼åŒ–è¾“å‡º\n",
    "print(f\"å…±è¯†åˆ«å‡º {len(grouped_topics)} ä¸ªä¸»é¢˜ç»„ï¼š\\n\")\n",
    "\n",
    "for _, row in grouped_topics.iterrows():\n",
    "    # å°† group ç¼–å·åŠ  1 æ–¹ä¾¿é˜…è¯»ï¼ˆä»ç¬¬ä¸€ç»„å¼€å§‹è®¡æ•°ï¼‰\n",
    "    group_num = row['group'] + 1\n",
    "    topic_list = row['topic_id']\n",
    "    \n",
    "    # æ‰“å°æ ¼å¼ï¼šç¬¬ä¸€ç»„ï¼š[0, 1, 2, ...]\n",
    "    print(f\"ç¬¬{group_num}ç»„ï¼š{topic_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. å¦‚æœä½ æƒ³åŒæ—¶çœ‹åˆ°è¿™ç»„ä¸»é¢˜çš„å…³é”®è¯æ ‡ç­¾ï¼ˆæ–¹ä¾¿éªŒè¯è¯­ä¹‰ä¸€è‡´æ€§ï¼‰\n",
    "print(\"\\n\" + \"=\"*30 + \"\\nè¯¦ç»†è¯­ä¹‰åˆ†ç»„ï¼š\\n\")\n",
    "for _, row in grouped_topics.iterrows():\n",
    "    group_num = row['group'] + 1\n",
    "    # è·å–è¯¥ç»„ä¸‹æ‰€æœ‰ä¸»é¢˜çš„æ ‡ç­¾\n",
    "    labels = df_res[df_res['group'] == row['group']]['label'].tolist()\n",
    "    print(f\"ç¬¬{group_num}ç»„ åŒ…å«ä¸»é¢˜ï¼š\")\n",
    "    for lbl in labels:\n",
    "        print(f\"  - {lbl}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. æå–åµŒå¥—åˆ—è¡¨æ ¼å¼ï¼š[[ç»„1åºå·], [ç»„2åºå·], ...]\n",
    "nested_topic_lists = grouped_topics['topic_id'].tolist()\n",
    "\n",
    "print(\"--- åµŒå¥—åˆ—è¡¨è¾“å‡º ---\")\n",
    "print(nested_topic_lists)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. åˆå¹¶åçš„ä¸»é¢˜ç†è§£æ–¹æ³•ä¸€ï¼šæå–æ¯ç»„çš„ä»£è¡¨æ€§å…³é”®è¯ï¼ˆä¸¢å¤±c-tf-idfæƒé‡ï¼‰\n",
    "# é€»è¾‘ï¼šè·å–æ¯ç»„å†…æ‰€æœ‰ä¸»é¢˜çš„å…³é”®è¯ï¼Œè®¡ç®—è¯é¢‘ï¼Œå–å‰ 10 ä¸ªä½œä¸ºç»„å…³é”®è¯\n",
    "group_keywords = {}\n",
    "\n",
    "for _, row in grouped_topics.iterrows():\n",
    "    group_num = row['group']\n",
    "    topics_in_group = row['topic_id']\n",
    "    \n",
    "    all_words_in_group = []\n",
    "    for t_id in topics_in_group:\n",
    "        # è·å–å•ä¸ªä¸»é¢˜çš„å…³é”®è¯ (è¿”å›çš„æ˜¯ [(word, score), ...])\n",
    "        words = [word for word, _ in topic_model.get_topic(t_id)]\n",
    "        all_words_in_group.extend(words)\n",
    "    \n",
    "    # ç»Ÿè®¡è¯é¢‘å¹¶å–å‰ 10 ä¸ªæœ€èƒ½ä»£è¡¨æœ¬ç»„çš„è¯\n",
    "    from collections import Counter\n",
    "    # è¿‡æ»¤æ‰ä¸€äº›æå…¶å¸¸è§çš„é‡å¤è¯ï¼Œä¿ç•™æ¯ç»„ç‹¬ç‰¹çš„è¯­ä¹‰\n",
    "    most_common_words = [word for word, count in Counter(all_words_in_group).most_common(10)]\n",
    "    group_keywords[group_num + 1] = most_common_words\n",
    "\n",
    "# 3. æ ¼å¼åŒ–å±•ç¤ºç»„å…³é”®è¯\n",
    "print(\"--- å„ç»„ä»£è¡¨æ€§å…³é”®è¯ ---\")\n",
    "for g_id, keywords in group_keywords.items():\n",
    "    print(f\"ç¬¬{g_id}ç»„å…±åŒç‰¹å¾è¯: {', '.join(keywords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆå¹¶ä¸»é¢˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå¹¶ä¸»é¢˜å,bertopic_modelä¼šæ”¹å˜ï¼Œå¦‚éœ€å¤šæ¬¡åˆå¹¶ï¼Œé‡æ–°è°ƒç”¨ä¿å­˜çš„åŸå§‹æ¨¡å‹\n",
    "topic_model.merge_topics(docs, nested_topic_lists)\n",
    "#topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å–æ‰€æœ‰ä¸»é¢˜çš„å­—å…¸ {topic_id: [(word, score), ...]}\n",
    "all_topics_dict = topic_model.get_topics()\n",
    "# è¿‡æ»¤æ‰ -1 (å™ªå£°)ï¼Œå¹¶å°†æ¯ä¸ªè¯åˆ—è¡¨è½¬ä¸ºé€—å·è¿æ¥çš„å­—ç¬¦ä¸²\n",
    "clean_topics_dict = {\n",
    "    topic_id: \",\".join([word for word, score in words_list])\n",
    "    for topic_id, words_list in all_topics_dict.items()\n",
    "    if topic_id != -1\n",
    "}\n",
    "\n",
    "for item in clean_topics_dict.items():\n",
    "    print(item)\n",
    "    \n",
    "# æ£€æŸ¥ç»“æœ\n",
    "print(f'å·²è·å–{len(clean_topics_dict)}ä¸ªä¸»é¢˜çš„è¯¦ç»†ä¿¡æ¯ï¼Œç°åœ¨é€šè¿‡AIåˆ¤æ–­ä¸»é¢˜åç§°â€¦â€¦') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIä¸»é¢˜è¯†åˆ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIè¯†åˆ«\n",
    "user_prompt = f\"è¯·åˆ†æä»¥ä¸‹ä¸»é¡Œå…³é”®è¯ï¼Œå¹¶è¿”å› JSON å­—å…¸ï¼š\\n{clean_topics_dict}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import re\n",
    "# åŠ è½½ .env æ–‡ä»¶\n",
    "load_dotenv(\".env\")\n",
    "API_KEY=os.environ.get('DEEPSEEK_API_KEY')\n",
    "deepseek_chat_model = \"deepseek-chat\" # DeepSeek-V3.2çš„éæ€è€ƒæ¨¡å¼\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=\"https://api.deepseek.com\")\n",
    "# AIè¯†åˆ«æ™ºèƒ½ä½“å‚æ•°è®¾ç½®ï¼šåˆ›å»ºä¸“é—¨ç”¨äºåœ°å€æ¨ç†çš„ LLM å®ä¾‹\n",
    "Deepseek_reasponse = client.chat.completions.create(\n",
    "    model=deepseek_chat_model,\n",
    "    messages=[ # å¯¹è¯æ¶ˆæ¯åˆ—è¡¨\n",
    "        {\"role\": \"system\", \"content\": system_prompt}, # ç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰åŠ©æ‰‹çš„è¡Œä¸º\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    "    response_format={'type': 'json_object'}, #å¼ºåˆ¶jsonæ ¼å¼è¿”å›\n",
    "    stream=False # éæµå¼å“åº”ï¼ˆä¸€æ¬¡æ€§è¿”å›å®Œæ•´ç»“æœï¼‰\n",
    ")\n",
    "content = json.loads(Deepseek_reasponse.choices[0].message.content)\n",
    "#content = re.sub(r\"^```json\\s*|\\s*```$\", \"\", content.strip())\n",
    "# å°†é”®è½¬ä¸ºæ•´å‹\n",
    "formatted_labels = {int(k): v for k, v in content.items()}\n",
    "print(formatted_labels)\n",
    "# ä½¿ç”¨è‡ªå®šä¹‰æ ‡ç­¾ï¼ˆéœ€è¦å…ˆè®¾ç½®ï¼‰\n",
    "topic_model.set_topic_labels(formatted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "h_data = {\n",
    "        \"n_groups\": n_groups,\n",
    "        \"all_topics\": all_topics # åŸå§‹ä¸»é¢˜åˆ—è¡¨\n",
    "    }\n",
    "\n",
    "# --- 0. åŸºç¡€è®¾ç½® ---\n",
    "title_base = f'{year_range}å¹´{data_source}è¿›ä¸€æ­¥å±‚æ¬¡èšç±»ç»“æœä¸€è§ˆè¡¨-{version}'\n",
    "output_dir = f\"BERTopic_Results_Allset_merged{n_groups}_{year_range}_{data_source}_{version}\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# --- å‚æ•°è¯´æ˜ ---\n",
    "report_name = os.path.join(output_dir, f\"report_{year_range}_{data_source}_{version}.html\")\n",
    "generate_bertopic_report(umap_cfg = umap_params, \n",
    "                         HDBSCAN_cfg=HDBSCAN_cfg,\n",
    "                         vectorizer_cfg = vectorizer_params,\n",
    "                         history= search_history,\n",
    "                         best_size = best_m_size,\n",
    "                         model_name = model_name, \n",
    "                         output_path = report_name,\n",
    "                         hierarchical_data = h_data)\n",
    "# print(f\"âœ” å‚æ•°è¯´æ˜æ–‡æ¡£å·²ç”Ÿæˆï¼š{report_name}\")\n",
    "\n",
    "# --- 1. ç”Ÿæˆå›¾è¡¨ (æŒ‰ç…§ä½ æƒ³è¦çš„é¡ºåº) ---\n",
    "#-------------------------------------------------------------------------\n",
    "# [å›¾ A] ä¸»é¢˜å…³é”®è¯æ¡å½¢å›¾ - å­˜ä¸º barchart.html\n",
    "fig_barchart = topic_model.visualize_barchart(\n",
    "    top_n_topics=len(formatted_labels),\n",
    "    custom_labels=True,\n",
    "    n_words=10,\n",
    "    height=400,\n",
    "    title=f\"{year_range}å¹´{data_source}å„ä¸»é¢˜å…³é”®è¯çš„c-TF-IDFæƒé‡å¾—åˆ†æ¡å½¢å›¾\",\n",
    ")\n",
    "\n",
    "fig_barchart.update_layout(\n",
    "    # å› ä¸ºæ¯è¡Œå¢åŠ äº†å­å›¾ï¼Œå»ºè®®å¢åŠ æ€»å®½åº¦ä»¥é˜²é‡å \n",
    "    width=1500, \n",
    "    # ç»Ÿä¸€å­—ä½“æ ·å¼\n",
    "    font=dict(family=\"KaiTi\", size=16),\n",
    "    title_font=dict(family=\"KaiTi\", size=36, color=\"black\", weight=\"bold\")\n",
    ")\n",
    "path_barchart = os.path.join(output_dir, \"barchart.html\")\n",
    "fig_barchart.write_html(path_barchart)\n",
    "#-------------------------------------------------------------------------\n",
    "# [å›¾ B] å±‚æ¬¡èšç±»å›¾ - å­˜ä¸º hierarchy.html\n",
    "fig_hierarchy = topic_model.visualize_hierarchy(\n",
    "    hierarchical_topics=hierarchical_topics,\n",
    "    custom_labels=True,\n",
    "    title=f\"{year_range}å¹´{data_source}ä¸»é¢˜å±‚æ¬¡èšç±»å›¾\",\n",
    "    height=800\n",
    ")\n",
    "fig_hierarchy.update_layout(\n",
    "    title_x=0.5,\n",
    "    # å› ä¸ºæ¯è¡Œå¢åŠ äº†å­å›¾ï¼Œå»ºè®®å¢åŠ æ€»å®½åº¦ä»¥é˜²é‡å \n",
    "    width=1500, \n",
    "    # ç»Ÿä¸€å­—ä½“æ ·å¼\n",
    "    font=dict(family=\"KaiTi\", size=16),\n",
    "    title_font=dict(family=\"KaiTi\", size=30, color=\"black\", weight=\"bold\")\n",
    ")\n",
    "path_hierarchy = os.path.join(output_dir, \"hierarchy.html\")\n",
    "fig_hierarchy.write_html(path_hierarchy)\n",
    "#-------------------------------------------------------------------------\n",
    "# [å›¾ C] æ–‡æ¡£åˆ†å¸ƒæ•£ç‚¹å›¾ - å­˜ä¸º documents.html\n",
    "fig_documents = topic_model.visualize_documents(\n",
    "    docs=[doc[:150] + \"...\" for doc in docs],\n",
    "    reduced_embeddings=reduced_embeddings,\n",
    "    custom_labels=True,\n",
    "    hide_document_hover=False\n",
    ")\n",
    "# æ›´æ–°æ•£ç‚¹å›¾æ ·å¼\n",
    "fig_documents.update_layout(\n",
    "    title=f\"{year_range}å¹´{data_source}ä¸»é¢˜åˆ†å¸ƒå›¾\",\n",
    "    title_x=0.5,\n",
    "    width=1500,\n",
    "    height=1200,\n",
    "    margin=dict(l=50, r=250, t=100, b=50),\n",
    "    font=dict(family=\"KaiTi\", size=16, color=\"black\"),\n",
    "    title_font=dict(family=\"KaiTi\", size=30, color=\"black\", weight=\"bold\")\n",
    ")\n",
    "path_documents = os.path.join(output_dir, \"documents.html\")\n",
    "fig_documents.write_html(path_documents)\n",
    "#-------------------------------------------------------------------------\n",
    "# [å›¾ D] ä¸»é¢˜æ—¶åºå›¾ - å­˜ä¸º topic_overtime_merged.html\n",
    "\n",
    "topics_over_time = topic_model.topics_over_time(docs, timestamps, global_tuning=False, evolution_tuning=False)\n",
    "fig_topic_time = topic_model.visualize_topics_over_time(\n",
    "    topics_over_time,\n",
    "    custom_labels=True)\n",
    "# æ›´æ–°æ•£ç‚¹å›¾æ ·å¼\n",
    "fig_topic_time.update_layout(\n",
    "    title=f\"{year_range}å¹´{data_source}ä¸»é¢˜æ—¶åºå›¾\",\n",
    "    title_x=0.5,\n",
    "    width=1500,\n",
    "    height=800,\n",
    "    font=dict(family=\"KaiTi\", size=16, color=\"black\"),\n",
    "    title_font=dict(family=\"KaiTi\", size=30, color=\"black\", weight=\"bold\")\n",
    ")\n",
    "path_topic_time = os.path.join(output_dir, \"topic_overtime_merged.html\")\n",
    "fig_topic_time.write_html(path_topic_time)\n",
    "# --- 2. åˆ›å»ºå¯¼èˆªç´¢å¼•é¡µ (index.html) ---\n",
    "# æ³¨æ„ï¼šæˆ‘åœ¨è¿™é‡ŒåŒæ­¥è°ƒæ•´äº†æŒ‰é’®é¡ºåºå’Œ iframe çš„é»˜è®¤ src\n",
    "index_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"zh-CN\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>{title_base}</title>\n",
    "    <style>\n",
    "        body {{ font-family: 'Microsoft YaHei', sans-serif; margin: 0; display: flex; flex-direction: column; height: 100vh; background-color: #f4f7f6; }}\n",
    "        header {{ background: #2c3e50; color: white; padding: 15px 25px; display: flex; justify-content: space-between; align-items: center; box-shadow: 0 2px 5px rgba(0,0,0,0.2); }}\n",
    "        h1 {{ margin: 0; font-size: 20px; }}\n",
    "        nav {{ background: #ecf0f1; padding: 10px; display: flex; gap: 10px; border-bottom: 1px solid #ddd; }}\n",
    "        .nav-btn {{ \n",
    "            padding: 8px 15px; background: white; border: 1px solid #bdc3c7; border-radius: 4px; \n",
    "            cursor: pointer; text-decoration: none; color: #34495e; font-size: 14px; transition: all 0.3s;\n",
    "        }}\n",
    "        .nav-btn:hover {{ background: #3498db; color: white; border-color: #2980b9; }}\n",
    "        .nav-btn.active {{ background: #3498db; color: white; }}\n",
    "        #content-frame {{ flex-grow: 1; border: none; width: 100%; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <header>\n",
    "        <h1>{title_base}</h1>\n",
    "        <span style=\"font-size: 12px; opacity: 0.8;\">ç‰ˆæœ¬: {version}</span>\n",
    "    </header>\n",
    "    \n",
    "    <nav>\n",
    "        <a class=\"nav-btn\" href=\"report_{year_range}_{data_source}_{version}.html\" target=\"chart_frame\">âœ¨ {year_range}å¹´{data_source}ä¸»é¢˜èšç±»å‚æ•°è¯´æ˜</a>\n",
    "        <a class=\"nav-btn\" href=\"barchart.html\" target=\"chart_frame\">ğŸ“ˆ  {year_range}å¹´{data_source}ä¸»é¢˜å…³é”®è¯æƒé‡å›¾</a>\n",
    "        <a class=\"nav-btn\" href=\"hierarchy.html\" target=\"chart_frame\">ğŸ“Š  {year_range}å¹´{data_source}ä¸»é¢˜å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾</a>\n",
    "        <a class=\"nav-btn\" href=\"documents.html\" target=\"chart_frame\">ğŸ“  {year_range}å¹´{data_source}ä¸»é¢˜åˆ†å¸ƒæ•£ç‚¹å›¾</a>\n",
    "        <a class=\"nav-btn\" href=\"topic_overtime_merged.html\" target=\"chart_frame\">âŒš {year_range}å¹´{data_source}ä¸»é¢˜æ—¶åºå›¾</a>\n",
    "    \n",
    "    </nav>\n",
    "\n",
    "    <iframe name=\"chart_frame\" id=\"content-frame\" src=\"report_{year_range}_{data_source}_{version}.html\"></iframe>\n",
    "\n",
    "    <script>\n",
    "        const buttons = document.querySelectorAll('.nav-btn');\n",
    "        buttons.forEach(btn => {{\n",
    "            btn.addEventListener('click', function() {{\n",
    "                buttons.forEach(b => b.classList.remove('active'));\n",
    "                this.classList.add('active');\n",
    "            }});\n",
    "        }});\n",
    "        // é»˜è®¤é«˜äº®ç¬¬ä¸€ä¸ªæŒ‰é’®ï¼ˆå³å‚æ•°è¯´æ˜é¡µé¢ï¼‰\n",
    "        buttons[0].classList.add('active');\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(output_dir, f\"index_{year_range}_{data_source}_res.html\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(index_content)\n",
    "\n",
    "print(f\"ğŸ‰ ç»“æœå¯è§†åŒ–å·²å®Œæˆï¼å·²è¯·æ‰“å¼€æ–‡ä»¶å¤¹: {output_dir}æŸ¥çœ‹\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
